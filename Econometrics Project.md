---
title: "ECONOMETRICS"
author: "Jad Istanbelly"
output:
  html_document:
    keep_md: true
    code_folding: "hide"
    toc: yes
    toc_float: yes
    toc_collapsed: yes
    toc_depth: 6
    theme: readable
  pdf_document:
    latex_engine: xelatex
urlcolor: blue
subtitle: 'Applying Statistical Methods to Datasets'
---



# Wages

The study of wage earning determination has a long history. According to one theory, the human capital
theory, the wage earning reflects the labor market rewards of human capital, a set of skills that a person
owns. The determination of the wage earning is usually estimated with log(wage) in the following based
model

$$
\ln \text { wage }=\beta_{1}+\beta_{2} \text { educ }+\beta_{3} \text { exper }+\beta_{4} \text { tenure }+\mathrm{e}
$$

Here: 

+ wage= Average hourly wage earning 
+ educ = years of education 
+ Exper= years of potential experience
+ Tenure = years with the current employer.


```r
remove(list = ls())
library(haven)
library(DT)

wage <- read_dta("C:/Users/jadel/OneDrive - Saint Marys University/Econometrics/Final/DataSets/wage.dta")
formatRound(datatable(wage), columns = "wage", digits = 2)
```

```{=html}
<div id="htmlwidget-a03f82ee3b6accbe7b31" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a03f82ee3b6accbe7b31">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526"],[3.09999990463257,3.24000000953674,3,6,5.30000019073486,8.75,11.25,5,3.59999990463257,18.1800003051758,6.25,8.13000011444092,8.77000045776367,5.5,22.2000007629395,17.3299999237061,7.5,10.6300001144409,3.59999990463257,4.5,6.88000011444092,8.47999954223633,6.32999992370605,0.529999971389771,6,9.5600004196167,7.78000020980835,12.5,12.5,3.25,13,4.5,9.68000030517578,5,4.67999982833862,4.26999998092651,6.15000009536743,3.50999999046326,3,6.25,7.80999994277954,10,4.5,4,6.38000011444092,13.6999998092651,1.66999995708466,2.9300000667572,3.65000009536743,2.90000009536743,1.62999999523163,8.60000038146973,5,6,2.5,3.25,3.40000009536743,10,21.6299991607666,4.38000011444092,11.710000038147,12.3900003433228,6.25,3.71000003814697,7.78000020980835,19.9799995422363,6.25,10,5.71000003814697,2,5.71000003814697,13.0799999237061,4.90999984741211,2.91000008583069,3.75,11.8999996185303,4,3.09999990463257,8.44999980926514,7.1399998664856,4.5,4.65000009536743,2.90000009536743,6.67000007629395,3.5,3.25999999046326,3.25,8,9.85000038146973,7.5,5.90999984741211,11.7600002288818,3,4.80999994277954,6.5,4,3.5,13.1599998474121,4.25,3.5,5.13000011444092,3.75,4.5,7.63000011444092,15,6.84999990463257,13.3299999237061,6.67000007629395,2.52999997138977,9.80000019073486,3.36999988555908,24.9799995422363,5.40000009536743,6.1100001335144,4.19999980926514,3.75,3.5,3.64000010490417,3.79999995231628,3,5,4.63000011444092,3,3.20000004768372,3.91000008583069,6.42999982833862,5.48000001907349,1.5,2.90000009536743,5,8.92000007629395,5,3.51999998092651,2.90000009536743,4.5,2.25,5,10,3.75,10,10.9499998092651,7.90000009536743,4.71999979019165,5.84000015258789,3.82999992370605,3.20000004768372,2,4.5,11.5500001907349,2.14000010490417,2.38000011444092,3.75,5.51999998092651,6.5,3.09999990463257,10,6.63000011444092,10,2.30999994277954,6.88000011444092,2.82999992370605,3.13000011444092,8,4.5,8.64999961853027,2,4.75,6.25,6,15.3800001144409,14.5799999237061,12.5,5.25,2.17000007629395,7.1399998664856,6.21999979019165,9,10,5.76999998092651,4,8.75,6.53000020980835,7.59999990463257,5,5,21.8600006103516,8.64000034332275,3.29999995231628,4.44000005722046,4.55000019073486,3.5,6.25,3.84999990463257,6.17999982833862,2.91000008583069,6.25,6.25,9.05000019073486,10,11.1099996566772,6.88000011444092,8.75,10,3.04999995231628,3,5.80000019073486,4.09999990463257,8,6.15000009536743,2.70000004768372,2.75,3,3,7.3600001335144,7.5,3.5,8.10000038146973,3.75,3.25,5.82999992370605,3.5,3.32999992370605,4,3.5,6.25,2.95000004768372,5.71000003814697,3,22.8600006103516,9,8.32999992370605,3,5.75,6.76000022888184,10,3,3.5,3.25,4,2.92000007629395,3.05999994277954,3.20000004768372,4.75,3,18.1599998474121,3.5,4.1100001335144,1.96000003814697,4.28999996185303,3,6.44999980926514,5.19999980926514,4.5,3.88000011444092,3.45000004768372,10.9099998474121,4.09999990463257,3,5.90000009536743,18,4,3,3.54999995231628,3,8.75,2.90000009536743,6.26000022888184,3.5,4.59999990463257,6,2.89000010490417,5.57999992370605,4,6,4.5,2.92000007629395,4.32999992370605,18.8899993896484,4.28000020980835,4.57000017166138,6.25,2.95000004768372,8.75,8.5,3.75,3.15000009536743,5,6.46000003814697,2,4.78999996185303,5.78000020980835,3.1800000667572,4.67999982833862,4.09999990463257,2.91000008583069,6,3.59999990463257,3.95000004768372,7,3,6.07999992370605,8.63000011444092,3,3.75,2.90000009536743,3,6.25,3.5,3,3.24000000953674,8.02000045776367,3.32999992370605,5.25,6.25,3.5,2.95000004768372,3,4.69000005722046,3.73000001907349,4,4,2.90000009536743,3.04999995231628,5.05000019073486,13.9499998092651,18.1599998474121,6.25,5.25,4.78999996185303,3.34999990463257,3,8.43000030517578,5.69999980926514,11.9799995422363,3.5,4.23999977111816,7,6,12.2200002670288,4.5,3,2.90000009536743,15,4,5.25,4,3.29999995231628,5.05000019073486,3.57999992370605,5,4.57000017166138,12.5,3.45000004768372,4.63000011444092,10,2.92000007629395,4.51000022888184,6.5,7.5,3.53999996185303,4.19999980926514,3.50999999046326,4.5,3.34999990463257,2.91000008583069,5.25,4.05000019073486,3.75,3.40000009536743,3,6.28999996185303,2.53999996185303,4.5,3.13000011444092,6.3600001335144,4.67999982833862,6.80000019073486,8.52999973297119,4.17000007629395,3.75,11.1000003814697,3.25999999046326,9.13000011444092,4.5,3,8.75,4.1399998664856,2.86999988555908,3.34999990463257,6.07999992370605,3,4.19999980926514,5.59999990463257,10,12.5,3.75999999046326,3.09999990463257,4.28999996185303,10.9200000762939,7.5,4.05000019073486,4.65000009536743,5,2.90000009536743,8,8.43000030517578,2.92000007629395,6.25,6.25,5.1100001335144,4,4.44000005722046,6.88000011444092,5.42999982833862,3,2.90000009536743,6.25,4.34000015258789,3.25,7.26000022888184,6.34999990463257,5.63000011444092,8.75,3.20000004768372,3,3,12.5,2.88000011444092,3.34999990463257,6.5,10.3800001144409,4.5,10,3.80999994277954,8.80000019073486,9.42000007629395,6.32999992370605,4,2.90000009536743,20,11.25,3.5,6,14.3800001144409,6.3600001335144,3.54999995231628,3,4.5,6.63000011444092,9.30000019073486,3,3.25,1.5,5.90000009536743,8,2.90000009536743,3.28999996185303,6.5,4,6,4.07999992370605,3.75,3.04999995231628,3.5,2.92000007629395,4.5,3.34999990463257,5.94999980926514,8,3,5,5.5,2.65000009536743,3,4.5,17.5,8.18000030517578,9.09000015258789,11.8199996948242,3.25,4.5,4.5,3.71000003814697,6.5,2.90000009536743,5.59999990463257,2.23000001907349,5,8.32999992370605,2.90000009536743,6.25,4.55000019073486,3.27999997138977,2.29999995231628,3.29999995231628,3.15000009536743,12.5,5.15000009536743,3.13000011444092,7.25,2.90000009536743,1.75,2.89000010490417,2.90000009536743,17.7099990844727,6.25,2.59999990463257,6.63000011444092,3.5,6.5,3,4.38000011444092,10,4.94999980926514,9,1.42999994754791,3.07999992370605,9.32999992370605,7.5,4.75,5.65000009536743,15,2.26999998092651,4.67000007629395,11.5600004196167,3.5],[11,12,11,8,12,16,18,12,12,17,16,13,12,12,12,16,12,13,12,12,12,12,16,12,11,16,16,16,15,8,14,14,13,12,12,16,12,4,14,12,12,12,14,11,13,15,10,12,14,12,12,16,12,12,12,15,16,8,18,16,13,14,10,10,14,14,16,12,16,12,16,17,12,12,12,13,12,12,12,18,9,16,10,12,12,12,12,12,8,12,12,14,12,12,12,9,13,12,14,12,15,12,12,12,14,15,12,12,12,17,11,18,12,14,14,10,14,12,15,8,16,14,15,12,18,16,10,8,10,11,18,15,12,11,12,12,14,16,2,14,16,12,12,13,12,15,10,12,16,13,9,12,13,12,12,14,16,16,9,18,10,10,13,12,18,13,12,13,13,13,18,12,12,13,12,12,12,14,10,12,16,16,12,14,12,12,12,12,12,12,12,16,16,14,11,16,12,12,17,12,12,16,8,12,12,12,16,12,12,9,13,16,14,8,14,13,12,18,9,8,8,12,14,12,16,8,13,9,16,12,15,11,14,12,12,12,18,12,12,12,12,12,12,14,16,12,14,11,12,10,12,6,13,12,10,12,14,13,12,18,12,12,12,12,12,8,13,13,14,12,10,16,12,16,12,14,18,17,13,14,15,14,12,8,12,12,8,12,9,12,16,12,16,12,12,13,10,6,12,12,16,12,8,12,6,4,11,11,7,12,18,12,16,12,14,12,10,10,9,10,12,12,12,10,16,16,16,12,12,7,8,16,16,18,13,10,16,14,16,12,9,11,11,12,11,12,12,12,12,14,14,18,12,12,12,11,12,17,16,13,13,12,14,14,11,10,8,14,12,10,17,9,12,12,14,16,12,10,0,14,15,16,12,11,11,12,13,12,13,16,15,16,15,12,18,6,6,12,12,16,9,12,11,10,12,8,9,17,16,11,10,8,13,14,13,11,7,16,12,13,14,16,14,11,8,14,17,10,12,12,18,14,18,12,16,14,12,9,12,12,17,12,15,17,16,12,15,16,12,15,12,12,12,12,16,11,14,14,13,14,12,12,8,12,3,11,15,11,12,4,9,12,12,11,12,16,13,15,16,12,12,12,9,10,12,11,8,6,16,12,12,16,12,10,13,13,14,16,10,12,12,11,0,5,16,16,9,15,12,12,12,13,12,7,17,12,12,14,12,13,12,16,10,15,16,14],[2,22,2,44,7,9,15,5,26,22,8,3,15,18,31,14,10,16,13,36,11,29,9,3,37,3,11,31,30,9,23,2,16,7,3,22,15,39,3,11,3,20,16,45,11,20,1,36,9,15,18,3,15,7,2,3,1,13,8,7,40,42,36,13,9,26,7,25,10,3,3,17,17,20,7,24,28,2,19,13,22,3,4,7,6,13,14,14,40,11,14,40,1,2,4,19,1,34,5,3,6,14,35,8,7,11,14,35,46,7,45,29,6,15,33,15,5,7,6,33,2,4,1,29,17,17,36,31,23,13,3,15,48,6,12,5,19,9,39,28,23,2,15,5,18,2,3,31,20,34,5,11,31,8,2,18,3,3,4,4,1,1,28,47,13,2,48,6,8,25,13,8,19,1,43,19,11,43,44,22,3,3,41,5,14,24,28,25,3,11,7,9,5,9,1,2,13,10,5,30,31,1,9,10,38,19,5,26,35,2,1,19,3,36,29,1,38,1,29,36,4,45,22,20,5,15,10,3,16,38,33,2,6,19,29,2,3,4,10,4,14,15,19,17,29,2,5,38,3,47,7,47,23,12,11,25,6,3,14,13,9,1,6,11,47,49,37,2,7,22,8,1,43,2,2,1,1,26,1,37,12,41,24,38,18,26,45,27,2,41,11,5,3,3,4,21,34,49,6,26,9,23,33,5,49,48,35,23,26,16,23,36,4,10,18,3,7,7,33,34,8,17,2,5,41,35,11,4,12,35,33,8,2,8,29,14,26,11,10,13,23,1,35,5,13,22,21,19,13,15,3,6,6,16,31,1,5,3,11,6,11,7,5,5,2,44,44,13,26,2,10,2,35,6,8,1,14,14,22,8,1,15,14,37,1,4,29,45,22,42,9,8,31,24,16,6,14,47,34,6,7,27,24,18,12,27,49,4,24,3,2,29,34,10,5,2,39,5,14,8,10,2,9,1,45,33,21,2,9,33,16,10,9,8,9,23,23,22,37,22,28,14,19,10,25,21,32,21,36,2,11,40,11,9,23,1,30,41,6,11,43,39,50,26,51,3,3,15,17,36,31,9,42,3,37,23,21,11,35,42,3,13,14,14,39,11,28,18,6,26,21,34,17,2,5,1,40,39,1,14,2,2,42,34,10,4,4,21,31,20,36,7,15,25,7,17,3,12,18,47,2,14,2,13,5,5],[0,2,0,28,2,8,7,3,4,21,2,0,0,3,15,0,0,10,0,6,4,13,9,1,8,3,10,0,0,1,5,5,16,3,0,4,6,15,3,0,0,5,0,12,4,13,0,2,2,1,0,2,5,7,0,0,1,0,8,0,20,5,8,0,3,23,4,3,5,2,0,2,8,34,0,19,0,1,13,0,5,1,0,5,2,3,0,4,24,7,6,39,0,0,1,1,0,22,2,0,6,0,12,4,7,3,11,10,0,0,12,25,3,0,16,0,0,2,1,12,1,0,0,0,3,3,3,30,2,1,3,0,1,0,0,0,5,3,13,11,20,0,1,0,2,2,0,4,5,15,0,0,3,5,2,5,0,2,1,4,0,0,5,4,1,6,2,5,0,21,7,1,10,4,5,9,5,4,3,11,2,2,11,0,11,16,8,8,0,0,6,2,0,3,0,1,0,2,3,8,19,2,0,0,0,6,0,2,12,0,2,10,2,24,24,2,3,2,0,15,0,4,3,4,0,2,2,0,7,1,26,0,5,3,0,0,1,0,1,0,10,5,0,0,7,0,0,3,0,0,6,13,2,3,0,23,0,1,7,0,0,0,0,1,44,6,17,0,0,8,0,1,6,2,1,3,0,20,1,7,4,23,1,26,0,1,2,0,0,8,4,0,1,2,0,13,26,6,5,9,0,9,2,2,7,0,31,2,1,0,3,8,0,0,2,1,0,7,2,12,0,1,0,0,16,28,4,0,3,0,0,6,0,10,1,5,3,3,2,0,20,2,31,2,11,3,9,0,0,5,0,2,5,1,2,0,2,0,0,7,3,2,0,4,2,7,25,0,15,1,3,0,0,5,1,0,10,6,10,4,4,5,12,10,1,4,0,8,0,10,0,0,15,24,5,0,0,25,5,2,4,2,5,0,3,3,0,0,2,0,0,11,21,3,0,0,21,2,2,2,1,2,3,0,1,3,18,0,1,2,2,0,8,1,1,0,8,18,0,4,25,0,4,9,0,0,0,10,0,2,0,2,1,7,4,0,13,33,0,0,17,2,24,20,30,9,1,9,6,0,9,4,10,0,14,22,5,12,13,0,0,0,7,11,1,8,3,0,2,1,6,2,2,0,0,0,30,21,1,5,2,1,0,0,3,3,0,3,3,14,1,0,0,17,0,0,1,11,5,1,0,2,0,18,1,4],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1],[1,1,0,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,1,1,1,0,1,1,1,0,0,0,0,1,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,0,0,1,0,1,1,1,0,0,0,0,1,0,1,0,1,0,0,1,0,0,1,0,0,1,1,0,0,1,1,0,1,1,0,0,0,0,1,0,1,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,1,1,1,1,0,1,0,0,1,0,0,0,0,0,1,0,1,0,0,1,1,1,0,1,0,1,1,0,1,0,0,0,0,1,0,1,0,0,0,1,0,0,1,1,1,1,1,1,1,1,0,0,0,1,1,0,0,0,1,0,0,1,0,1,1,1,1,1,0,0,1,0,1,1,1,1,1,0,0,1,0,1,1,0,1,0,0,0,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,1,1,1,1,0,1,1,1,1,1,0,0,0,1,0,0,1,0,1,1,1,0,1,1,1,0,0,0,0,0,0,0,0,1,0,1,1,1,1,0,1,0,1,0,1,1,0,0,0,1,0,1,0,1,0,0,1,0,0,1,1,0,1,1,1,1,0,1,0,1,0,1,0,0,0,1,1,1,0,0,0,1,0,0,0,0,0,0,1,1,1,0,0,1,0,0,1,0,0,0,1,0,0,0,1,1,1,1,1,0,0,0,0,1,0,1,0,1,1,1,1,0,0,0,0,1,1,0,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,1,1,0,0,0,0,1,1,0,1,0,0,0,1,1,1,0,1,0,0,0,1,1,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,1,0,1,1,1,0,0,1,1,0,0,0,1,1,0,1,1,0,0,1,1,0,1,1,1,0,1,0,0,0,1,1,0,1,1,0,1,0,0,0,1,1,0,1,0,0,0,0,0,1,0,1,0,0,1,0,0,1,1,0,0,1,0,0,1,0,1,1,1,1,0,1,0,0,1,0,1,1,1,1,0,0,1,0,0,0,0,0,1,1,1,0,0,0,0,1,0,0,0,0,1,0,0,1,0,1,0,0,1,1,1,1,0,1,0,0,1,0,1,1,0,0,0,1,1,0,1,0,0,0,1,1,0,0,0,0,0,1,1,0,0,1],[0,1,0,1,1,1,0,0,0,1,0,0,1,0,1,1,1,0,1,1,0,1,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,0,1,0,1,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,0,1,1,1,0,0,0,0,1,1,0,1,1,1,1,0,0,0,1,0,1,1,0,0,1,1,0,1,1,1,0,0,1,1,1,1,1,1,0,1,0,1,1,1,1,0,0,1,1,1,0,0,1,1,0,1,0,1,0,1,1,1,1,1,0,0,1,0,0,0,1,1,1,0,0,0,1,0,1,1,1,0,0,0,0,1,1,0,0,1,1,1,1,0,0,1,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,0,1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,1,0,1,1,0,1,0,1,1,0,1,0,1,1,1,1,0,1,1,1,0,1,1,1,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,1,1,0,1,0,0,1,0,1,0,0,0,1,1,1,0,1,0,1,0,0,1,1,0,0,0,0,1,0,1,1,1,0,0,1,1,0,1,0,1,1,1,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,1,1,1,0,1,1,0,1,0,0,1,1,1,0,0,1,1,0,0,1,0,1,1,0,1,1,1,1,1,0,1,1,0,1,1,1,1,1,1,0,1,1,0,1,0,1,0,0,1,1,1,1,1,1,1,0,1,1,1,1,0,1,0,1,1,1,0,1,1,0,1,0,1,1,1,0,1,1,1,0,0,0,1,0,1,1,0,0,1,1,1,1,1,1,1,0,1,0,0,1,0,0,1,1,1,0,0,1,0,1,0,1,1,1,0,1,1,1,0,1,1,1,1,0,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,0,1,1,0,1,1,0,0,0,1,0,1,1,1,0,1,0,0,1,1,1,1,1,1,1,0,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,0,0,0,0,1,0,0,1,0,0,1,1,1,1,0,1,0,1,0,1,0,1,1,1,0,1,1,1,0,1,0,1,1,0],[2,3,2,0,1,0,0,0,2,0,0,0,2,0,1,1,0,0,3,0,0,3,0,0,0,1,1,0,2,2,2,2,0,0,0,1,1,5,0,0,0,4,1,0,0,2,1,1,0,2,2,0,1,0,2,1,1,0,0,0,0,0,0,4,0,2,3,3,1,4,0,3,2,2,0,2,1,1,4,2,4,0,1,0,1,1,1,1,2,1,2,0,2,0,0,1,1,0,2,0,0,3,0,0,1,2,2,0,0,0,0,0,0,2,0,0,0,0,1,3,0,2,0,1,2,2,0,0,2,0,0,0,0,1,3,2,4,2,0,0,4,0,0,1,3,2,5,3,3,0,5,1,0,0,1,2,0,0,5,0,4,1,1,0,0,0,0,1,2,2,0,0,2,1,0,1,0,0,0,2,1,0,0,0,0,3,0,1,0,0,0,1,0,0,3,0,0,3,0,0,3,0,0,2,0,2,0,1,0,1,1,0,0,3,1,3,1,1,1,0,3,0,0,2,0,1,0,0,2,0,1,0,0,2,2,0,0,0,0,1,3,1,3,4,1,2,0,0,0,0,0,0,0,3,2,0,0,0,2,0,0,0,1,2,0,1,0,0,0,2,1,0,0,0,2,0,0,2,2,0,2,0,0,0,4,0,0,3,3,0,2,0,0,0,0,4,0,6,1,0,0,1,0,1,0,0,0,2,3,2,1,1,0,3,1,1,2,0,1,0,2,2,1,0,0,1,0,0,3,1,1,0,0,2,0,1,1,2,0,0,2,2,0,0,2,3,2,3,0,2,0,0,0,0,0,1,0,0,0,0,2,0,0,0,2,1,0,3,1,1,1,1,1,1,0,0,2,0,0,0,2,2,3,2,1,0,0,0,2,0,0,1,2,0,1,4,5,2,0,1,1,3,0,2,1,3,0,0,2,0,1,3,0,1,1,2,0,0,2,0,2,0,1,0,0,0,3,3,0,0,1,0,0,2,0,2,2,0,0,1,1,2,2,1,1,1,0,1,0,0,2,0,2,1,0,1,0,0,1,0,0,0,0,0,0,0,2,1,2,1,6,1,0,2,0,3,0,1,0,0,0,0,2,3,0,2,4,3,0,4,3,0,0,4,0,4,0,0,2,3,0,2,2,5,1,0,1,2,1,3,0,1,1,5,2,2,0,3,2,0,0,2,3,3,0,2],[1,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,0,0,1,1,1,0,0,1,0,0,0,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,0,0,1,1,1,1,0,1,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,1,1,0,1,1,1,1,0,0,1,1,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,1,0,1,0,1,1,1,1,1,0,0,1,1,1,1,1,1,0,1,1,0,0,1,1,1,0,0,0,0,0,1,1,0,1,1,1,1,1,1,1,0,0,0,0,0,1,1,1,1,0,0,1,1,1,1,1,1,1,1,1,0,0,1,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,1,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,0,0,1,1,1,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,1,0,1,0,0,1,0,1,0,0,1,0,1,0,1,0,0,0,0,1,0,1,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,1,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0,1,0,0,0,1,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,1,0,1,1,1,0,0,1,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,1,0,1,1,1,0,0,0,1,1,0,1,1,0,0,1,0,1,0,1,1,0,1,0,0,0,0,0,0,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,1,0,0,0,1,0,1,0,1,1,0,0,1,0,1,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,1,1,0,0,1,0,0,0,1,0,0,0,1,0,1,1,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,1,1,0,0,1,0,0,0],[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,1,0,1,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,1,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,1,1,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,1,1,0,1,1,1,0,0,1,0,0,0,1,1,0,0,1,0,0,1,0,1,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,1,0,0,1,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,0,1,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,1,0,1,0,0,0,1,0,0,0,0,0,1,1,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,1,0,0,1,1,0,0,1,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,1,0,1,1,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,1,1,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,1,0,1,0,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,1,0,0,0,1,0,0,0,1],[0,0,0,0,0,1,1,1,1,1,1,0,0,0,1,1,0,1,0,0,1,0,1,0,0,1,0,1,1,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,0,1,0,1,1,0,0,1,1,1,1,1,0,0,1,0,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,0,0,1,1,0,1,0,1,1,0,0,1,0,0,0,0,1,1,0,1,1,1,1,0,1,1,0,0,0,0,0,1,0,1,0,1,1,0,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,0,1,1,0,1,0,0,1,0,1,1,0,0,0,0,0,1,1,1,0,1,1,1,0,0,1,1,0,1,0,0,1,0,1,1,1,0,1,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,0,0,1,0,0,1,0,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,1,1,1,0,1,1,1,0,0,1,0,0,0,1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,1,0,0,0,1,1,1,0,1,0,0,1,1,1,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,1,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,1,0,1,1,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,1,1,1,0,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,1,0,1,0,1,0,1,0,1,1,0,0,0,1,1,0,0,1,0,0,1,0,0,0,1,1,1,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,0,1,0,1,1,0,0,0,1,1,1,0,0],[0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,0,0,0,1,1,1,0,0,1,1,0,0,0,0,0,0,1,0,0,0,0,1,1,0,1,0,0,1,0,0,0,0,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,1,1,0,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>wage<\/th>\n      <th>educ<\/th>\n      <th>exper<\/th>\n      <th>tenure<\/th>\n      <th>nonwhite<\/th>\n      <th>female<\/th>\n      <th>married<\/th>\n      <th>numdep<\/th>\n      <th>smsa<\/th>\n      <th>northcen<\/th>\n      <th>south<\/th>\n      <th>west<\/th>\n      <th>construc<\/th>\n      <th>ndurman<\/th>\n      <th>trcommpu<\/th>\n      <th>trade<\/th>\n      <th>services<\/th>\n      <th>profserv<\/th>\n      <th>profocc<\/th>\n      <th>clerocc<\/th>\n      <th>servocc<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"targets":1,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 2, 3, \",\", \".\");\n  }"},{"className":"dt-right","targets":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":["options.columnDefs.0.render"],"jsHooks":[]}</script>
```


## Estimating Models and Interpreting Results

Using Linear Algebra we can form different models and interpret their Betas and R2. Betas can signify whether a positive or negative relationship exists between the dependent and independent variable. Meanwhile R2 signifies the variance explained by the dependent variables in a model.

We will create three models: 

1. A "Base" model consisting of simple regression
$$
\ln \text { wage }=\beta_{1}+\beta_{2} \text { educ }+\beta_{3} \text { exper }+\beta_{4} \text { tenure }+\mathrm{e}
$$
2. An "Adjusted" model consisting of a squared variation of simple regression
$$
\ln \text { wage }=\beta_{1}+\beta_{2} \text { educ }+\beta_{3} \text { exper }^2+\beta_{4} \text { tenure }^2+\mathrm{e}
$$
3. An "Expanded" model consisting of simple regression with more dependent variables
$$
\ln \text { wage }=\beta_{1}+\beta_{2} \text { educ }+\beta_{3} \text { exper }+\beta_{4} \text { tenure }+\beta_{5} \text { nonwhite }+\beta_{6} \text { female }+\beta_{7} \text { married } +\mathrm{e}
$$

### Base Model

The code chunk below solves for simple regression using Linear Algebra.


```r
df <- wage[, c(1:4)] # Extract necessary variables and input in a DF
df[, 1] <- log(df[, 1]) # Convert to log of wage
df_matrix <- as.matrix(df) # Convert DF to Matrix for Linear Algebra Calculations

x_1 <- 1 # Create Intercept

X <- cbind(x_1, df_matrix[ , c(2:4)]) # Create a X matrix
y <- (df_matrix[, 1]) # Create a Y matrix

betahat <- solve(t(X)%*%X)%*%t(X)%*%y # Solve for Beta-hats
yhat <- X%*%betahat # Solve for Y-hats
uhat <- y-yhat # Solve for U-hats

model1 <- lm(wage ~ educ+exper+tenure, df) # Use Regression function to check Linear Algebra work calculations

SSE <- t(uhat)%*%(uhat) # Solve for Sum of Squared Estimate of Errors aka Residual Sum of Squares
TSS <- t(y)%*%y-nrow(X)*mean(y)^2 # Solve for Total Sum of Squares 
SSR <- TSS - SSE # Solve for Sum of Squares Regression aka Explained Sum of Squares

R2 <- SSR/TSS # Solve for R-Squared aka Coefficient of determination
adj_R2 <- 1-((nrow(df)-1)/(nrow(df)-ncol(df)))*(1-R2) # Solve for Adjusted R-Squared
```

#### Beta 
Positive relationships exist between the dependent variables and Wage

```r
betahat
```

```
##               [,1]
## x_1    0.284359555
## educ   0.092028987
## exper  0.004121109
## tenure 0.022067217
```

#### R2
The dependent variables in this model can explain around 31% of our independent variable (wage)

```r
library(scales)
percent(adj_R2, accuracy = 0.01)
```

```
## [1] "31.21%"
```

### "Adjusted" Squared Variation of Model 

The code chunk below solves for squared regression using Linear Algebra.


```r
x_3 <- wage[, 3]^2 # Square third variable
x_4 <- wage[, 4]^2 # Square fourth variable

df2 <- cbind(wage[, c(1, 2)], x_3, x_4) # Combine all variable into one DF
df2[, 1] <- log(df2[, 1]) # Log of independent variable
df2_matrix <- as.matrix(df2) # Convert DF to Matrix for Linear Algebra Calculations


X2 <- cbind(x_1, df2_matrix[ , c(2:4)]) # Create a X matrix
y2 <- (df2_matrix[, 1]) # Create a Y matrix 

betahat2 <- solve(t(X2)%*%X2)%*%t(X2)%*%y2 # Solve for beta-hats
yhat2 <- X2%*%betahat2 # Solve for Y-hats
uhat2 <- y2-yhat2 # Solve for U-hats

model2 <- lm(wage ~ educ+exper+tenure, df2) # Use Regression function to check Linear Algebra calculations

SSE2 <- t(uhat2)%*%(uhat2) # Solve for Sum of Squared Estimate of Errors aka Residual Sum of Squares
TSS2 <- t(y2)%*%y2-nrow(X2)*mean(y2)^2 # Solve for Total Sum of Squares
SSR2 <- TSS2 - SSE2 # Solve for Sum of Squares Regression aka Explained Sum of Squares

R2_2 <- SSR2/TSS2 # Solve for R-Squared aka Coefficient of determination
adj_R2_2 <- 1-((nrow(df2)-1)/(nrow(df2)-ncol(df2)))*(1-R2_2) # Solve for Adjusted R-Squared
```

#### Beta
Positive relationships exist between the dependent variables and Wage

```r
betahat2
```

```
##                [,1]
## x_1    0.3949407570
## educ   0.0911712337
## exper  0.0000721750
## tenure 0.0006244027
```

#### R2
The dependent variables in this model can explain around 25.77% of our independent variable (wage)

```r
percent(adj_R2_2, accuracy = 0.01)
```

```
## [1] "25.77%"
```


### Is Exper and Tenure Linearly Related to Wage?

We can compare R2 between the Base model and the Adjusted model to answer this question (see code chunk)

```r
# If statement that determines which R2 is greater 
if(adj_R2 > adj_R2_2){
  print("Wage is linearly related to Exper and Tenure")
} else {print("Wage is not linearly related to Exper and Tenure")}
```

```
## [1] "Wage is linearly related to Exper and Tenure"
```


### Expanded Variation of Model

The code chunk below solves for simple expanded regression using Linear Algebra.


```r
df3 <- wage[, c(1:7)] # Combine all variables into one DF
df3[, 1] <- log(df3[, 1]) # Log of independent variable
df3_matrix <- as.matrix(df3) # Convert DF to matrix for Linear Algebra calculations

X3 <- cbind(x_1, df3_matrix[ , c(2:7)]) # Create a X matrix
y3 <- (df3_matrix[, 1]) # Create a Y matrix

betahat3 <- solve(t(X3)%*%X3)%*%t(X3)%*%y3 # Solve for beta-hats
yhat3 <- X3%*%betahat3 # Solve for Y-hats
uhat3 <- y3-yhat3 # Solve for U-hats


model3 <- lm(wage ~., df3) # Use regression function to check Linear Algebra calculations

SSE3 <- t(uhat3)%*%(uhat3) # Solve for Sum of Squared Estimate of Errors aka Residual Sum of Squares
TSS3 <- t(y3)%*%y3-nrow(X3)*mean(y3)^2 # Solve for Total Sum of Squares
SSR3 <- TSS3 - SSE3 # Solve for Sum of Squares Regression aka Explained Sum of Squares

R2_3 <- SSR3/TSS3 # Solve for R-Squared aka Coefficient of determination
adj_R2_3 <- 1-((nrow(df3)-1)/(nrow(df3)-ncol(df3)))*(1-R2_3) # Solve for Adjusted R-Squared
```

#### Beta
Nonwhite and female have a negative relationship to wage, all other variables show a positive relationship

```r
betahat3
```

```
##                  [,1]
## x_1       0.490670265
## educ      0.083882687
## exper     0.003134062
## tenure    0.016868862
## nonwhite -0.002527438
## female   -0.285568314
## married   0.125636410
```

#### R2
The dependent variables in this model can explain around 39.67% of our independent variable (wage)

```r
percent(adj_R2_3, accuracy = 0.01)
```

```
## [1] "39.67%"
```



### Is the Expanded model acceptable? Does it fit our data of Wages?

To answer these questions we have to compare the joint significance of the extra variables found in the expanded model vs our base model and determine if the extra variables have an affect to our Y (Wage). By looking at the Residual Sum of Squares aka "the unexplained portion of the variance", computed through the F-statistic.
The hypotheses are stated below:

$$
H_{0}: \beta_{1}=\cdots=\beta_{p}=0
$$
$$
H_{1}: \text { At least one of } \beta_{1}, \ldots, \beta_{p} \neq 0
$$






```r
F_statistic <- ((SSE-SSE3)/3)/(SSE3/(nrow(X3)-ncol(X3))) # Manual Computation for F-Stat

F_critical <- qf(.99, df1 = 3, df2 = (nrow(X3)-ncol(X3))) # Manual Computation for F Crtit at 99 confidence level

# Hypothesis Test
if(F_statistic > F_critical){
  print("We reject the Null, there is sufficient evidence to support the expanded model")
} else {print("We fail to reject the Null, there is insufficient evidence to support the expanded model")}
```

```
## [1] "We reject the Null, there is sufficient evidence to support the expanded model"
```

Another method is to compare the computed p-value to a significance level (alpha) rather than comparing F-values


```r
# Create alpha level

a <- 0.01

# Run Comparison between both models and extract p-value

pval <- anova(model1, model3)$`Pr(>F)`[2]

# Hypothesis Test

if (pval < a){
  print("We reject the Null, there is sufficient evidence to support the expanded model")
} else {print("We fail to reject the Null, there is insufficient evidence to support the expanded model")}
```

```
## [1] "We reject the Null, there is sufficient evidence to support the expanded model"
```

## Heteroskedasticity and Auto-Correlation in the Accepted Model

### Testing for Heteroskedasticity 

We must test for Heteroskedasticity and insure that the unexplained portion of the variance is Homoskedastic, constant across observations (according to OLS). If our model is Heteroskedastic then our model is no longer BLUE specifically, no longer the best model to fit our data. We can run a Breaush-Pagan test to check for Heteroskedasticity, the hypotheses are listed below:


$$ H_0: var\left(\hat{u}_{i}\right) = var\left(\hat{u}_{j}\right) \text { for } i = j $$
$$ H_1: var\left(\hat{u}_{i}\right) \neq var\left(\hat{u}_{j}\right) \text { for } i \neq j $$

#### Breausch-Pagan Test

```r
library(lmtest)

# Set Alpha level

alpha <- 0.05

# Extract P-Value from BP Test

p <- bptest(model3)$p.value 

# Hypothesis Test

if(p < alpha){
  print("We reject the Null and conclude that there is heteroskedasticity in model")
  } else {print("We fail to reject the Null and conclude that there is no heteroskedasticity in model")}
```

```
## [1] "We fail to reject the Null and conclude that there is no heteroskedasticity in model"
```

### Testing for Serial Correlation

Main assumptions of linear regression refer to no correlation between residuals meaning that they are independent. We can run a Durbin-Watson test to check for Auto-correlation, the hypotheses are listed blow: 

$$ H_0: \rho_{1}=\rho_{2}=\cdots=\rho_{p}=0$$

$$ H_1: \rho_{1}\neq\rho_{2}\neq\cdots\neq\rho_{p}\neq0$$


##### Durbin-Watson Test

```r
# Set Alpha level
alpha <- 0.05

# Extract Statistic from DW test

dw <- dwtest(model3)$statistic 

# Check DW Statistic

if( 1.5 < dw && dw < 2.5){
  print("Relatively normal value")
} else {print("Potentially concerning value")}
```

```
## [1] "Relatively normal value"
```

```r
# Extract P-value from test

pval_dw <- dwtest(model3)$p.value 

# Hypothesis Test

if(pval_dw < alpha){
  print("We reject the Null and conclude that there is auto-correlation in the residuals of our model")
  } else {print("We fail to reject the Null and conclude that there is no auto-correlation in the residuals of our model")}
```

```
## [1] "We reject the Null and conclude that there is auto-correlation in the residuals of our model"
```



#### Corrected Standard Errors

If Heteroskedacsticty or Serial Correlation is found, then it is likely that our standard errors are biased or inflated or both. Let us look at the difference between our original and robust Standard Errors


```r
library(sandwich)

# Corrected SE

SE <- coeftest(model3, vcov = vcovHC(model3, "HC1"))[, 2] 

# Original SE

SE_org <- summary(model3)$coefficients[,2] 

# Difference in SE

SE_final <- SE - SE_org 

# Compile into a basic Matrix

SE_table <- format(rbind(SE, SE_org, SE_final), scientific = F)
`rownames<-`(SE_table, c("Robust", "Original", "Difference"))
```

```
##            (Intercept)      educ             exper            tenure           nonwhite         female           married         
## Robust     " 0.11587764283" " 0.00802930658" " 0.00162136292" " 0.00355062847" " 0.06202077573" " 0.03785614279" " 0.04035782120"
## Original   " 0.10223094554" " 0.00699961670" " 0.00168368218" " 0.00295858711" " 0.05965028300" " 0.03731126552" " 0.04009740652"
## Difference " 0.01364669729" " 0.00102968988" "-0.00006231926" " 0.00059204136" " 0.00237049273" " 0.00054487727" " 0.00026041468"
```

Based on our hypothesis testing we know that we do not have an alarming level of auto-correlation. This is further proven by the very small difference between the original and correct Standard Errors


## Under-fitted and Over-fitted Models

Let us suppose that the true model is: 
$$
\ln \text { wage }=\beta_{1}+\beta_{2} e d u c+\beta_{3} \text { exper }+e
$$
and we have estimated two models: 

1) Under-fitted Model
$$
\ln \text { wage }=\alpha_{1}+\alpha_{2} e d u c+v
$$
2) Over-fitted Model
$$
\ln \text { wage }=\beta_{1}+\beta_{2} e d u c+\beta_{3} \text { exper }+\beta_{4}\text {xexper} +e
$$
Show that the bias is: 

$$
\hat{\alpha}_{2}-\beta_{2}=\beta_{3}[\operatorname{cov}(e d u c, \ exper) / \operatorname{var}(educ)]
$$

### Estimating True Model

```r
# Similar steps for Linear Algebra calculations (see "Estimating Models and Interpreting Results" section)

dft <- wage[, c(1:3)]
dft[, 1] <- log(dft[, 1])
dft_matrix <- as.matrix(dft)

Xt <- cbind(x_1, dft_matrix[ , c(2, 3)])
yt <- (dft_matrix[, 1])

betahatt <- solve(t(Xt)%*%Xt)%*%t(Xt)%*%yt
yhatt <- Xt%*%betahatt
uhatt <- yt-yhatt

true_model <- lm(wage ~. , dft)
```

### Estimating Under-fitted Model

```r
# Similar steps for Linear Algebra calculations (see "Estimating Models and Interpreting Results" section)

dfu <- wage[, c(1, 2)]
dfu[, 1] <- log(dfu[, 1])
dfu_matrix <- as.matrix(dfu)

Xu <- cbind(x_1, dfu_matrix[ , 2])
yu <- (dfu_matrix[, 1])

betahatu <- solve(t(Xu)%*%Xu)%*%t(Xu)%*%yu

under_model <- lm(wage ~. , dfu)
```

### Proof of Bias

```r
covt <- (t(Xt)%*%Xt)/(nrow(dft)) # Covariance Table of True Model

cov_edu_exper <- covt[2, 3] # Extract covariance of specifc variable
var_edu <- covt[2, 2] # Extract variance of specific variable

b3 <- betahatt[3, ] # Extract Beta3 from True model
b2 <- betahatt[2, ] # Extract Beta2 from True model
a2 <- betahatu[2, ] # Extract Beta2 from Underfitted model


tol <- 1e-2 # Using tolerance of 1% to account to any minor fluctuations in calculations

# If both sides are equal to each other then one side minus the other should approx equal to zero

if (abs(a2-b2) - abs(b3 * (cov_edu_exper/var_edu)) <= tol){ 
  print("Proof of bias exists and is approx equal to each other")
} else print("Proof of bias does not exist and not approx equal to each other")
```

```
## [1] "Proof of bias exists and is approx equal to each other"
```

### Estimating Over-fitted Model

```r
xexper <- rnorm(526, 50, 2) # Extra variable, this is given

# Similar steps for Linear Algebra calculations (see "Estimating Models and Interpreting Results" section)

dfo <-cbind(wage[, c(1:3)], xexper)
dfo[, 1] <- log(dfo[, 1])
dfo_matrix <- as.matrix(dfo)

Xo <- cbind(x_1, dfo_matrix[ ,c(2:4)])
yo <- (dfo_matrix[, 1])

betahato <- solve(t(Xo)%*%Xo)%*%t(Xo)%*%yo
yhato <- Xo%*%betahato
uhato <- yo-yhato


over_model <- lm(wage ~. , dfo)
```


### Over-fitted Standard Errors

Incorporating a random variable has inflated our intercept by around 40%. This means that if we were to estimate the wage of a person who does not have any experience or education we would be off by 40% from our estimation with the true model.

```r
sigma2t <- sum(uhatt^2)/(nrow(dft)-ncol(dft)) # True sigma
sigma2o <- sum(uhato^2)/(nrow(dfo)-ncol(dfo)) # Overfitted sigma

vcmt <- sigma2t*solve(t(Xt)%*%Xt) # True Variance-Covarivance Matrix
vcmo <- sigma2o*solve(t(Xo)%*%Xo) # Overfitted Variance-Covariance Matrix

vart <- diag(vcmt) # True variance
varo <- diag(vcmo) # Overfitted variance

SE_t <- sqrt(vart) # True Standard Errors
SE_o <- sqrt(varo) # Overfitted Standard Errors

SE_t <- c(SE_t, 0) # Adding a zero in place of missing variable from overfitted model (xexper)

diff_SE <- format(abs(SE_o - SE_t) , scientific = FALSE,) # Difference in Standard Errors

dt_SE <- cbind.data.frame(SE_t, SE_o, diff_SE) # Combine results into a DF
dt_SE <-`row.names<-.data.frame`(dt_SE,c("x1", "educ", "exper", "xexper")) # Change row names 
dt_SE <- `colnames<-`(dt_SE, c("True SE", "Overfitted SE", "Difference")) # Change column names

formatPercentage(datatable(dt_SE), columns = c("True SE", "Overfitted SE", "Difference"), digits = 2) # Round numbers for easy read
```

```{=html}
<div id="htmlwidget-126398508d013a2d0ef4" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-126398508d013a2d0ef4">{"x":{"filter":"none","vertical":false,"data":[["x1","educ","exper","xexper"],[0.108595022043288,0.00762239752634345,0.0015551386167852,0],[0.521681017322524,0.00762897401072731,0.00155626627393939,0.0102066702293952],["0.413085995279","0.000006576484","0.000001127657","0.010206670229"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>True SE<\/th>\n      <th>Overfitted SE<\/th>\n      <th>Difference<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"targets":1,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatPercentage(data, 2, 3, \",\", \".\");\n  }"},{"targets":2,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatPercentage(data, 2, 3, \",\", \".\");\n  }"},{"targets":3,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatPercentage(data, 2, 3, \",\", \".\");\n  }"},{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":["options.columnDefs.0.render","options.columnDefs.1.render","options.columnDefs.2.render"],"jsHooks":[]}</script>
```




# CAPM

The Capital Asset Pricing Model is a common method to explain variations in the rate of return on a security as a function of the rate of returen on a market protfolio. 
In this dataset, we are given:

+ the monthly returns of six firms (dis, gm ... etc.)
+ rate of return on the market (mkt)
+ the risk free return (rf)


```r
remove(list = ls())

library(haven)
capm <- read_dta("C:/Users/jadel/OneDrive - Saint Marys University/Econometrics/Final/DataSets/capm.dta")
formatRound(datatable(capm), columns = colnames(capm), digits = 3)
```

```{=html}
<div id="htmlwidget-5f7b036ac54a2e5a5a43" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-5f7b036ac54a2e5a5a43">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132"],[0.0808840021491051,0.0473680011928082,-0.0463429987430573,0.168337002396584,-0.090818002820015,-0.0722960010170937,-0.0151580004021525,-0.203266993165016,-0.0751710012555122,0.0636449977755547,0.194895997643471,-0.0679609999060631,0.100000001490116,0.0662880018353462,-0.115452997386456,0.0200800001621246,-0.082676999270916,0.0579399988055229,-0.105476997792721,0.00680299988016486,-0.0630630031228065,0.0192310009151697,0.0598109997808933,0.0493270009756088,0.241453006863594,-0.0774530023336411,0.231343001127243,0.0500000007450581,-0.0317460000514984,-0.0745159983634949,-0.00322100007906556,0.0064619998447597,-0.0176570005714893,-0.0637250021100044,-0.191972002387047,0.00725700007751584,0.052267998456955,0.0164199993014336,-0.0759290009737015,0.0576919987797737,0.0452889986336231,-0.086337998509407,-0.0879200026392937,-0.0349150002002716,-0.267794013023376,-0.00161100004334003,0.101130001246929,0.0224719997495413,0.0164090003818274,0.092118002474308,0.00347799994051456,0.00433300016447902,-0.0116480002179742,-0.175033003091812,-0.06190500035882,-0.115622997283936,-0.0344390012323856,0.103037998080254,0.186826005578041,-0.166498005390167,0.0729610025882721,-0.0251429993659258,-0.00234499992802739,0.0963570028543472,0.0530549995601177,0.00508899986743927,0.109872996807098,-0.0647810027003288,-0.0160980001091957,0.122459001839161,0.0198759995400906,0.01948899962008,0.0287180002778769,0.105416998267174,-0.0580479986965656,-0.0784310027956963,0.0191050004214048,0.0860669985413551,-0.0941549986600876,-0.0277180001139641,0.0044539999216795,0.118404000997543,0.065820999443531,0.0431549996137619,0.0298560000956059,-0.0241010002791882,0.0282749999314547,-0.0811000019311905,0.0393939986824989,-0.0823620036244392,0.0182680003345013,-0.0175509992986917,-0.0420800000429153,0.00994599983096123,0.0229790005832911,-0.0276779998093843,0.055902998894453,0.10588700324297,-0.0035729999653995,0.00251000002026558,0.0908439978957176,-0.0163930002599955,-0.0103329997509718,-0.00134700001217425,0.0424959994852543,0.0177939999848604,0.0505400002002716,0.0462939999997616,0.0262620002031326,-0.0261589996516705,0.00525500020012259,0.0159740000963211,0.013150000013411,-0.0230819992721081,-0.0333920009434223,0.0181820001453161,0.0235120002180338,0.00697899982333183,-0.0427369996905327,-0.0156859997659922,-0.0755890011787415,0.0861259996891022,-0.0317800007760525,0.0334610007703304,0.0360779985785484,-0.0714289993047714,-0.0272439997643232,0.0658980011940002,-0.0513140000402927,-0.155751004815102,-0.130836993455887,0.0230909995734692],[0.0562179982662201,0.00322599988430738,0.112378999590874,-0.0116029996424913,-0.0212769992649555,0.0899550020694733,-0.012516999617219,-0.105521000921726,-0.00171900005079806,0.099763996899128,0.0328570008277893,0.132503002882004,0.0281860008835793,-0.043503999710083,0.106293000280857,-0.0474580004811287,-0.0349939987063408,0.111248001456261,-0.0323010012507439,0.0303899999707937,0.0587650015950203,0.142857000231743,-0.0396680012345314,0.192392006516457,-0.136510998010635,-0.00935500022023916,0.178735002875328,0.0104419998824596,0.00516699999570847,0.00593100022524595,-0.0269029997289181,0.140947997570038,-0.0147110000252724,-0.0498370006680489,-0.095780998468399,-0.0295589994639158,-0.0408339984714985,0.0113089997321367,-0.0963440015912056,0.159340992569923,0.00968500040471554,-0.00510200019925833,-0.104410000145435,-0.0579309985041618,-0.0883359983563423,-0.0212370008230209,0.0574019998311996,0.0457139983773232,-0.0731040015816689,0.0411840006709099,-0.0272729992866516,-0.157543003559113,-0.0129949999973178,-0.0613359995186329,0.108433999121189,-0.0636650025844574,-0.176450997591019,0.0243410002440214,0.0740590021014214,-0.0951329991221428,-0.0496920011937618,0.0475369989871979,0.0602909997105598,0.154901996254921,-0.0254670009016991,0.00592300016433001,-0.00836800038814545,0.039733000099659,0.0145420003682375,-0.0268370006233454,-0.0117199998348951,0.0875480026006699,0.0855389982461929,-0.0270589999854565,-0.0615010000765324,-0.0186759997159243,0.0390649996697903,0.0475579984486103,0.0262349992990494,-0.0138349998742342,0.0301920007914305,0.0160809997469187,0.0363419987261295,0.0384620018303394,-0.0101370001211762,-0.0196509994566441,0.0244319997727871,0.00388199998997152,0.00773499999195337,-0.0441339984536171,-0.00432900013402104,-0.0257970001548529,0.00833099987357855,0.00712799979373813,0.0533769987523556,-0.0117579996585846,-0.0656210035085678,0.0112979998812079,0.0581079982221127,-0.00546299992129207,-0.00953999999910593,-0.0306480005383492,-0.00819199997931719,0.0419090017676353,0.0437459982931614,-0.00538200000301003,0.0048420000821352,0.0626420006155968,-0.0311740003526211,-0.0238559991121292,0.0128899998962879,0.0424209982156754,0.0195330008864403,0.026078000664711,0.0125390002503991,0.00283799995668232,0.0722920000553131,-0.00579700013622642,-0.0697280019521713,-0.0237659998238087,-0.0461289994418621,-0.054016001522541,0.116777002811432,-0.116455003619194,-0.0605509988963604,-0.12109400331974,0.0599480010569096,-0.00671599991619587,-0.081495001912117,-0.234901994466782,-0.119938999414444,-0.0384389981627464],[-0.0462960004806519,0.198489993810654,-0.0172259993851185,-0.0055350000038743,0.0742119997739792,-0.0704350024461746,0.0823199972510338,-0.18928299844265,-0.0559139996767044,0.151481002569199,0.113748997449875,0.024150000885129,0.254148006439209,-0.0745130032300949,0.0543529987335205,0.0237070005387068,-0.0712530016899109,-0.04347800090909,-0.0738639980554581,0.0920249968767166,-0.0500000007450581,0.119166001677513,0.0292809996753931,0.00954900030046701,0.108341000974178,-0.049651000648737,0.0887430012226105,0.130566000938416,-0.24031999707222,-0.177875995635986,-0.0193760003894567,0.276618987321854,-0.0995670035481453,-0.0442310012876987,-0.195170998573303,0.0290399994701147,0.0542329996824265,0.0022350000217557,-0.0275689996778965,0.0570879988372326,0.0472539998590946,0.130931004881859,-0.011655000038445,-0.131289005279541,-0.216437995433807,-0.0368300005793571,0.214908003807068,-0.0221330001950264,0.0522629991173744,0.0457569994032383,0.140996992588043,0.0612079985439777,-0.0233830008655787,-0.139983996748924,-0.129093006253242,0.0388830006122589,-0.187213003635406,-0.145244002342224,0.209022998809814,-0.0715370029211044,-0.0143790002912283,-0.0567019991576672,-0.00444199983030558,0.0722780004143715,-0.00610300013795495,0.0189640000462532,0.0397219993174076,0.11140800267458,-0.00413599982857704,0.0425119996070862,0.014295999892056,0.248246997594833,-0.0696630030870438,-0.0213370006531477,-0.0211970005184412,0.00679400004446507,-0.0322650000452995,0.0264379996806383,-0.0740500018000603,-0.0308299995958805,0.0283220000565052,-0.0925140008330345,0.014007999561727,0.0380930006504059,-0.0811280012130737,-0.0179299991577864,-0.175595998764038,-0.0922079980373383,0.200525000691414,0.0783379971981049,0.0829410031437874,-0.0578490011394024,-0.10470899939537,-0.104868002235889,-0.182482004165649,-0.113242000341415,0.238929003477097,-0.14546999335289,0.0472670011222363,0.0756929963827133,0.187937006354332,0.106201000511646,0.0819069966673851,-0.0868759974837303,0.139822006225586,0.0499100014567375,-0.155784994363785,0.0509749986231327,0.0690099969506264,-0.0210110004991293,-0.0394980013370514,0.0192559994757175,-0.031700000166893,0.260419994592667,-0.142857000231743,-0.0435190014541149,0.193884000182152,0.0678469985723495,-0.232456997036934,-0.165604993700981,0.133386999368668,-0.165898993611336,-0.181701004505157,0.217848002910614,-0.252155005931854,-0.327484995126724,-0.0373909994959831,-0.0966579988598824,-0.0549999997019768,-0.38730201125145,-0.094990998506546,-0.389313012361526],[-0.0561529994010925,0.0596200004220009,-0.00538600003346801,0.115523003041744,0.01592200063169,-0.0228719990700483,0.154055997729301,-0.148340001702309,0.140954002737999,0.155642002820969,0.113434001803398,0.116577997803688,-0.00610199989750981,-0.0724690034985542,0.0441830009222031,0.180182993412018,0.110200002789497,0.114224001765251,-0.0275630000978708,-0.00799600034952164,-0.0285999998450279,-0.188016995787621,0.0502040013670921,0.0466950014233589,0.0405559986829758,-0.0835629999637604,0.148417994379997,-0.0550849996507168,-0.0358299985527992,0.0203729998320341,0.0262410007417202,0.175142005085945,-0.147726997733116,-0.124444000422955,-0.0494420006871223,-0.0909089967608452,0.317647010087967,-0.106875002384186,-0.037236999720335,0.197129994630814,-0.0277919992804527,0.0107330000028014,-0.0689380019903183,-0.0486649982631207,-0.0823410004377365,0.178259998559952,0.0708800032734871,0.0464570000767708,-0.108052000403404,-0.089258000254631,0.0599270015954971,-0.194615006446838,-0.0377269983291626,-0.105034001171589,-0.0222219992429018,0.0728690028190613,-0.226453006267548,0.353798985481262,0.102990001440048,-0.108374997973442,0.0090319998562336,-0.00127899996004999,0.00615799985826015,0.0824939981102943,0.0388690009713173,-0.0629260018467903,-0.0151519998908043,0.0113230003044009,0.0770640000700951,0.0130190001800656,0.0136339999735355,0.0236360002309084,0.0706729963421822,-0.0258990004658699,-0.0482899993658066,-0.0399609990417957,0.00680499989539385,-0.00496699986979365,-0.0122520001605153,-0.0252669993788004,0.0123979998752475,0.0467690005898476,0.0520329996943474,0.0460529997944832,-0.0523429997265339,-0.00706500001251698,-0.0129620004445314,-0.16414999961853,-0.00824800040572882,-0.0178689993917942,0.124797999858856,-0.0316319987177849,-0.00496200006455183,0.0206930004060268,0.0881780013442039,-0.0753659978508949,-0.0109489997848868,-0.0105779999867082,0.0277919992804527,-0.0015760000096634,-0.0259899999946356,-0.0385480001568794,0.00767999980598688,0.0498640015721321,0.0119799999520183,0.126800000667572,-0.0011909999884665,0.0568969994783401,0.0205869991332293,-0.0596069991588593,0.0142029998824,0.0843409970402718,0.0468639992177486,-0.0126639995723963,0.0513060018420219,0.0582019984722137,0.00951199978590012,-0.0142609998583794,-0.090768001973629,0.0277619995176792,-0.00915800034999847,0.0667539983987808,0.0112420003861189,0.0482890009880066,0.0764710009098053,-0.0842150002717972,0.0797270014882088,-0.044929001480341,-0.0391849987208843,-0.205112993717194,-0.116919003427029,0.0313730016350746],[0.154255002737045,0.136153995990753,0.0560469999909401,0.00698299985378981,-0.0589459985494614,0.277819007635117,0.0144180003553629,-0.127344995737076,0.147230997681618,-0.0380470007658005,0.152301996946335,0.136783003807068,0.261830002069473,-0.142142996191978,0.194004997611046,-0.0927480012178421,-0.00768599985167384,0.117738001048565,-0.0485100001096725,0.0786599963903427,-0.0216070003807545,0.0220839995890856,-0.0163739994168282,0.282305985689163,-0.161669999361038,-0.0868450030684471,0.188811004161835,-0.343528985977173,-0.103046998381615,0.278721004724503,-0.127343997359276,0,-0.136078998446465,0.141968995332718,-0.166969001293182,-0.244009003043175,0.407781004905701,-0.0337769985198975,-0.0730929970741272,0.238857001066208,0.0211069993674755,0.0552179999649525,-0.0932879969477654,-0.138087004423141,-0.103068001568317,0.136408001184464,0.104212999343872,0.0317710004746914,-0.0383399985730648,-0.0842880010604858,0.0337679982185364,-0.133477002382278,-0.0258319992572069,0.0744450017809868,-0.122851997613907,0.0229260008782148,-0.108801998198032,0.222451001405716,0.0787359997630119,-0.103675000369549,-0.0820119976997375,0.00210699997842312,0.0215189997106791,0.0557620003819466,-0.0371670015156269,0.0418529994785786,0.0300309993326664,0.00416499981656671,0.0482649989426136,-0.053957000374794,-0.0164500009268522,0.0645660012960434,0.0102300001308322,-0.0405060015618801,-0.060309000313282,0.0481350012123585,0.00382700003683567,0.0888300016522408,-0.00245100003667176,-0.0389610007405281,0.0128210000693798,0.0115729998797178,0.0686450004577637,-0.00335699995048344,-0.0164669994264841,-0.0395740009844303,-0.0393479987978935,0.0467519983649254,0.0229250006377697,-0.0372090004384518,0.0309980008751154,0.0722369998693466,-0.0602630004286766,-0.00116600003093481,0.080155998468399,-0.0552750006318092,0.0764819979667664,-0.0422729998826981,0.0126529997214675,-0.112458996474743,-0.0583849996328354,0.0286979991942644,0.0326180011034012,0.0719040036201477,0.0642020031809807,0.0497259981930256,0.0261230003088713,0.0170300006866455,0.0334899984300137,-0.0839269980788231,-0.0106499996036291,0.0742729976773262,0.0283930003643036,-0.0397560000419617,-0.016287999227643,-0.00551899988204241,0.0254089999943972,0.249491006135941,-0.0842159986495972,0.0595239996910095,-0.0842700004577637,-0.16227300465107,0.0433860011398792,0.00493300007656217,-0.00315600004978478,-0.0286020003259182,-0.0650670006871223,0.0653190016746521,-0.0219860002398491,-0.163357004523277,-0.0886700004339218,-0.0385759994387627],[-0.0306439995765686,0.081727996468544,0.060784000903368,0.0804070010781288,-0.0294610001146793,0.0124110002070665,-0.0157619994133711,-0.06266900151968,0.0792739987373352,0.0141589995473623,0.0528450012207031,-0.025000000372529,-0.0393159985542297,-0.046654999256134,0.0600939989089966,0.177147999405861,-0.0334389992058277,-0.0344289988279343,0.0291729997843504,-0.00113400002010167,-0.0364499986171722,-0.025492999702692,0.0768269971013069,0.015760000795126,0.0356869995594025,-0.0921050012111664,0.0348550006747246,-0.00320799998007715,0.0780690014362335,-0.0577640011906624,0.0191079992800951,0.0258120000362396,0.0918840020895004,0.000700999982655048,-0.00838099978864193,-0.0120740002021194,-0.0320629999041557,-0.0316100008785725,-0.000616999983321875,0.0938270017504692,0.00665900018066168,-0.0155490003526211,-0.0438469983637333,-0.0330459997057915,-0.0186800006777048,0.00126900000032037,-0.0461339987814426,0.0508019998669624,-0.0063609997741878,0.0635079964995384,0.061259001493454,-0.0835049971938133,-0.000249000004259869,0.0247930008918047,-0.101662002503872,-0.0293799992650747,-0.100140996277332,0.0551720000803471,0.0407009981572628,0.00402300013229251,-0.0226099994033575,0.0029279999434948,0.0273369997739792,0.0071530002169311,0.041193000972271,-0.0134619995951653,-0.00918999966233969,0.0666100010275841,-0.0291779991239309,-0.000546000024769455,-0.00355400005355477,0.132596999406815,-0.00512199988588691,0.0399609990417957,-0.0137539999559522,0.0230819992721081,0.0227969996631145,0.0268210005015135,0.0425579994916916,0.00151199998799711,0.0483729988336563,0.0184150002896786,0.0467289984226227,0.000195000000530854,0.00663299998268485,0.23217099905014,-0.0586009994149208,-0.0431209988892078,-0.00946899969130754,0.0225980002433062,0.0222720000892878,0.0245110001415014,0.0607680007815361,-0.116461999714375,0.0388309992849827,-0.0320519991219044,0.117144003510475,-0.0487649999558926,0.0250969994813204,0.0364769995212555,-0.0293279998004436,0.0072240000590682,0.104156002402306,0.00369099993258715,-0.00842299964278936,0.0643820017576218,0.0799499973654747,-0.0023429999127984,-0.033015999943018,-0.0283400006592274,0.0525950007140636,0.0520870015025139,0.0521540008485317,0.00853700004518032,0.0149020003154874,0.0111589999869466,0.0796689987182617,-0.00615799985826015,-0.0269590001553297,0.0508080013096333,-0.0778099969029427,0.0111109996214509,-0.0279280003160238,0.10037799924612,-0.0420110002160072,-0.00709800003096461,-0.0873709991574287,-0.000249000004259869,-0.0293710008263588,-0.0455829985439777,0.0867509990930557,-0.00399299990385771],[0.00452899979427457,0.0732299983501434,0.0513219982385635,0.0108620002865791,-0.0257549993693829,0.031954001635313,-0.0232640001922846,-0.157666996121407,0.0638360008597374,0.0743570029735565,0.0619859993457794,0.0630529969930649,0.0383460000157356,-0.0381049998104572,0.0378619991242886,0.0489999987185001,-0.0207199994474649,0.0510200001299381,-0.0306349992752075,-0.00998300034552813,-0.0228669997304678,0.0620479993522167,0.0368559993803501,0.0839250013232231,-0.0397530011832714,0.0317699983716011,0.0535520017147064,-0.0594669990241528,-0.0390469990670681,0.0516490004956722,-0.0171050000935793,0.0758109986782074,-0.0511390008032322,-0.0245639998465776,-0.102545998990536,0.020346000790596,0.0395330004394054,-0.0992669984698296,-0.0702899992465973,0.0839039981365204,0.0105609996244311,-0.0174809992313385,-0.0183210000395775,-0.0590800009667873,-0.0915400013327599,0.0279670003801584,0.078734003007412,0.017841000109911,-0.0160600002855062,-0.021704999729991,0.0446930006146431,-0.0496490001678467,-0.0104580000042915,-0.0702430009841919,-0.0811370015144348,0.00798400025814772,-0.0999749973416328,0.0749579966068268,0.0612759999930859,-0.0533090010285378,-0.023429999127984,-0.0154079999774694,0.0103329997509718,0.08279699832201,0.0635069981217384,0.0163320004940033,0.0231299996376038,0.0249080006033182,-0.00910199992358685,0.0603309981524944,0.0166069995611906,0.0455319993197918,0.0230640005320311,0.0154670001938939,-0.0106849996373057,-0.0242299996316433,0.0141270002350211,0.0215629991143942,-0.0376809984445572,0.00271399994380772,0.0205550007522106,0.0178100001066923,0.0482430011034012,0.0351799987256527,-0.0265570003539324,0.0226949993520975,-0.0169370006769896,-0.0251819994300604,0.0379160009324551,0.0115310000255704,0.0433310009539127,-0.00594299985095859,0.0106089999899268,-0.0207839999347925,0.0404059998691082,0.00347599992528558,0.0401119999587536,-0.00164399994537234,0.0190600007772446,0.0129680000245571,-0.0310420002788305,-0.000395999988541007,-0.00190200004726648,0.025085000321269,0.0194520000368357,0.0370859988033772,0.0237160008400679,0.0108230002224445,0.0194380003958941,-0.0139979999512434,0.0129490001127124,0.0398930013179779,0.0388940013945103,-0.0147580001503229,-0.0317529998719692,0.0116229997947812,0.0409029982984066,0.0258419997990131,-0.0492790006101131,-0.00434200000017881,-0.062304999679327,-0.0220400001853704,-0.0104700000956655,0.0511440001428127,0.0238249991089106,-0.078625001013279,-0.013150000013411,0.0110419997945428,-0.0980599969625473,-0.184725999832153,-0.085206001996994,0.0214820001274347],[0.0502500012516975,0.0512199997901917,0.0522999987006187,0.0472799986600876,0.045669998973608,0.0470299981534481,0.0474499985575676,0.046909999102354,0.0402799993753433,0.0355199985206127,0.0444200001657009,0.0439400002360344,0.044089999049902,0.0450600013136864,0.0455799996852875,0.0445200018584728,0.0441999994218349,0.0443800017237663,0.0445000007748604,0.0450000017881393,0.0450100004673004,0.0436599999666214,0.0457100011408329,0.0510200001299381,0.0538600012660027,0.0531399995088577,0.0585599988698959,0.0527000017464161,0.0467500016093254,0.0562900006771088,0.0587499998509884,0.0623399987816811,0.0592000000178814,0.0602700002491474,0.0611900016665459,0.0576899982988834,0.0482699982821941,0.0501299984753132,0.044319998472929,0.0374200008809566,0.0344499982893467,0.0341500006616116,0.0364000014960766,0.0337399989366531,0.0237799994647503,0.0209299996495247,0.0172799993306398,0.0163599997758865,0.016890000551939,0.0173300001770258,0.0171000007539988,0.0175299998372793,0.016890000551939,0.016669999808073,0.017000000923872,0.0165599994361401,0.0156999994069338,0.0142000000923872,0.0121700000017881,0.0113599998876452,0.011459999717772,0.0119500001892447,0.0114099998027086,0.0109299998730421,0.0112100001424551,0.00793999992311001,0.00879999995231628,0.00970999989658594,0.00851999968290329,0.00915000028908253,0.00899999961256981,0.00827000010758638,0.00827000010758638,0.00932999979704618,0.00922999996691942,0.00808999966830015,0.00908000022172928,0.011289999820292,0.0127800004556775,0.0144100002944469,0.014580000191927,0.0174800008535385,0.0203200001269579,0.0190799999982119,0.021619999781251,0.0249700006097555,0.0259899999946356,0.0271099992096424,0.0275100003927946,0.0298500005155802,0.0322900004684925,0.0334100015461445,0.0318600013852119,0.0369599983096123,0.0397100001573563,0.0394599996507168,0.0432699993252754,0.0442400015890598,0.0459299981594086,0.0461300015449524,0.0469500012695789,0.046840000897646,0.0504000000655651,0.0496499985456467,0.0464899986982346,0.0515099987387657,0.0498000010848045,0.0476800017058849,0.0500099994242191,0.0514300018548965,0.0505100004374981,0.0475500002503395,0.0469500012695789,0.0453100018203259,0.0510499998927116,0.0415199995040894,0.0344300009310246,0.0398200005292892,0.0321899987757206,0.0285800006240606,0.0166400000452995,0.0207000002264977,0.0123800002038479,0.0125799998641014,0.0194799993187189,0.0169399995356798,0.0158300008624792,0.0166400000452995,0.00963999982923269,0.00254000001586974,0.000300000014249235,0.000300000014249235]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>dis<\/th>\n      <th>ge<\/th>\n      <th>gm<\/th>\n      <th>ibm<\/th>\n      <th>msft<\/th>\n      <th>xom<\/th>\n      <th>mkt<\/th>\n      <th>rf<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"targets":1,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\");\n  }"},{"targets":2,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\");\n  }"},{"targets":3,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\");\n  }"},{"targets":4,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\");\n  }"},{"targets":5,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\");\n  }"},{"targets":6,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\");\n  }"},{"targets":7,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\");\n  }"},{"targets":8,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatRound(data, 3, 3, \",\", \".\");\n  }"},{"className":"dt-right","targets":[1,2,3,4,5,6,7,8]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":["options.columnDefs.0.render","options.columnDefs.1.render","options.columnDefs.2.render","options.columnDefs.3.render","options.columnDefs.4.render","options.columnDefs.5.render","options.columnDefs.6.render","options.columnDefs.7.render"],"jsHooks":[]}</script>
```

The CAPM formula is:

$$
r p r=\gamma+\beta(m p r)+e
$$
Where:
$$
rpr = r-r_m
$$
$$
mpr = r_m- r_f
$$

## Estimating Models for each Firm

First, we should calculate mpr, then rpr for all firms. Then create simple regressions for each firm. See code chunk below:


```r
mpr <- capm[, 7] - capm[, 8] # calculating mpr
colnames(mpr) <- "mpr" # renaming colnames for mpr

# Calculating rpr for each firm

rpr_dis <- capm[, 1] - capm[, 8] # Disney
rpr_ge <- capm[, 2] - capm[, 8] # GE
rpr_gm <- capm[, 3] - capm[, 8] # GM
rpr_ibm <- capm[, 4] - capm[, 8] # IBM
rpr_msft <- capm[, 5] - capm[, 8] # Microsoft
rpr_xom <- capm[, 6] - capm[, 8] # Mobil-Exxon

# Input each rpr into a DF

df <- cbind(rpr_dis, rpr_ge, rpr_gm, rpr_ibm, rpr_msft, rpr_xom, mpr)

# Create regression functions

model_dis <- lm(dis ~ mpr, df )
model_ge <- lm(ge ~ mpr, df )
model_gm <- lm(gm ~ mpr, df )
model_ibm <- lm(ibm ~ mpr, df )
model_msft <- lm(msft ~ mpr, df )
model_xom <- lm(xom ~ mpr, df )
```

## Estimated Beta Values of Each Firm 

The Beta of a stock can inform you of its volatility relative to the market. 
Agressive meaning riskier and more volatile meanwhile, Defensive meaning safer and less volatile.

In formula notation it can be represented as follows: 

$$
Agressive = \beta_{market} < \beta_{stock}
$$

$$
Defensive = \beta_{market} > \beta_{stock}
$$



```r
# Extracting Beta Coeffecients from each regression function and rounding for a cleaner number

beta <- round(cbind(model_dis$coefficients[2], model_ge$coefficients[2], 
              model_gm$coefficients[2], model_ibm$coefficients[2], 
              model_msft$coefficients[2], model_xom$coefficients[2]), 4)

# Renaming Columns for each Firm and their Beta Coeffecient

colnames(beta) <- c("Disney", "GE", "GM", "IBM", "Microsoft", "Mobile-Exxon")
risk <- c("Defensive", "Defensive", "Aggressive", "Aggressive", "Aggressive", 
          "Defensive" )

# Combine the Betas and the coressponding risk status to each firm into a simple matrix

risk_status <- rbind(beta, risk)
risk_status
```

```
##      Disney      GE          GM           IBM          Microsoft    Mobile-Exxon
## mpr  "0.9146"    "0.859"     "1.1468"     "1.1482"     "1.2599"     "0.4613"    
## risk "Defensive" "Defensive" "Aggressive" "Aggressive" "Aggressive" "Defensive"
```

## Does CAPM hold in our regression models?

We can answer this question using a simple F-test for each firm and test if gamma (the intercept) equals to zero. The Hypotheses are listed below:

$$
H_{0}: \ \gamma=0
$$
$$
H_{1}: \gamma \neq 0 
$$




```r
# Load Required Package
library(car)

# Set Alpha level
alpha <- 0.05

# Create Hypothesis Statement
hypo <- "(Intercept)=0"

# Compute and Extract F-Statistic
fstat_dis <- linearHypothesis(model_dis, hypothesis.matrix =  hypo)$`Pr(>F)`[2]
fstat_ge <- linearHypothesis(model_ge, hypo)$`Pr(>F)`[2]
fstat_gm <- linearHypothesis(model_gm, hypo)$`Pr(>F)`[2]
fstat_ibm <- linearHypothesis(model_ibm, hypo)$`Pr(>F)`[2]
fstat_msft <- linearHypothesis(model_msft, hypo)$`Pr(>F)`[2]
fstat_xom <- linearHypothesis(model_xom, hypo)$`Pr(>F)`[2]

# Find P-Value for given F-stat and DF
pval_dis <- 1-pf(fstat_dis, 1, model_dis$df.residual)
pval_ge <- 1-pf(fstat_ge, 1, model_ge$df.residual)
pval_gm <- 1-pf(fstat_gm, 1, model_gm$df.residual)
pval_ibm <- 1-pf(fstat_ibm, 1, model_ibm$df.residual)
pval_msft <- 1-pf(fstat_msft, 1, model_msft$df.residual)
pval_xom <- 1-pf(fstat_xom, 1, model_xom$df.residual)
```


```r
# If Statements for Final Results
if(pval_dis < alpha){
  print("We reject the Null and conclude that the model is significantly different from 0. Therefore, CAPM does not hold for Disney")
}else {print("We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for Disney.")}
```

```
## [1] "We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for Disney."
```

```r
if(pval_ge < alpha){
  print("We reject the Null and conclude that the model is significantly different from 0. Therefore, CAPM does not hold for GE")
}else {print("We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for GE.")}
```

```
## [1] "We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for GE."
```

```r
if(pval_gm < alpha){
  print("We reject the Null and conclude that the model is significantly different from 0. Therefore, CAPM does not hold for GM")
}else {print("We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for GM.")}
```

```
## [1] "We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for GM."
```

```r
if(pval_ibm < alpha){
  print("We reject the Null and conclude that the model is significantly different from 0. Therefore, CAPM does not hold for IBM")
}else {print("We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for IBM.")}
```

```
## [1] "We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for IBM."
```

```r
if(pval_msft < alpha){
  print("We reject the Null and conclude that the model is significantly different from 0. Therefore, CAPM does not hold for Microsoft")
}else {print("We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for Microsoft.")}
```

```
## [1] "We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for Microsoft."
```

```r
if(pval_xom < alpha){
  print("We reject the Null and conclude that the model is significantly different from 0. Therefore, CAPM does not hold for Mobil-Exxon")
}else {print("We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for Mobil-Exxon.")}
```

```
## [1] "We fail to reject the Null and conclude that the model is not significantly different from 0. Therefore, CAPM does hold for Mobil-Exxon."
```

# Money

The link between Inflation and supply of money has been studied for a long time. Based off of one theory, Inflation is linked to the rate of money growth and the rate at which output grows. Thus leading to the following equation: 

$$
INF = \beta_1 + \beta_2GM + \beta_3GX + e
$$
Where: 

+ INF = rate of inflation (%)
+ GM = rate of growth of money supply (%)
+ GX= rate of growth of real GDP (%)


```r
remove(list = ls())

library(haven)
money <- read_dta("C:/Users/jadel/OneDrive - Saint Marys University/Econometrics/Final/DataSets/money.dta")
datatable(money)
```

```{=html}
<div id="htmlwidget-6c3f3c348e533237825c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-6c3f3c348e533237825c">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76"],[356.7,11.5,7.3,18,6.3,207.1,25.2,3.1,8.1,3.5,5.6,29.1,4.9,25.5,9.8,29.3,38.8,21.4,18.5,13,11.4,3.1,19,6.8,42,22.3,19.3,59.8,14.3,16.7,26.3,18.4,7.4,78.6,10.5,2.5,27.3,8.2,16.4,21.9,16.3,16.4,18.6,12.6,7.7,10.9,21.6,57.8,13.7,15.8,19.9,3.9,18,9.5,14.2,8.7,36.9,296.6,17.4,18.5,8.1,4.7,57.2,13.5,16.6,11.6,15.6,6.9,6.2,19.2,3.2,15.5,60.6,7.2,70.1,22.7],[374.3,6.1,3.6,8.6,1.4,187.1,12.3,4,3.9,4.2,0.7,20.1,-0.6,22.1,4.6,25,40.4,13.6,17,2.8,5.8,1.5,16.2,2.7,37,17.3,16.8,58.7,8.2,8.7,8.5,17.1,4.8,70.4,8.8,1.5,22.4,1.5,9.9,6.3,13.8,16.1,15.5,2.2,4.4,8.2,8.8,57.9,6.6,16.5,11.5,1.3,20.6,4.6,7.4,4.8,25,316.1,13.6,16.4,3.4,4.9,61.6,2.5,14.7,8.4,11.1,6.9,3.8,4.3,3.7,7.1,53.5,3.8,66.7,23.9],[0.8,3.1,2.3,4.2,2.7,1.1,9.6,0,2.6,1,4.8,5.1,2.7,3.6,2,2.8,2.4,4.3,1.6,1.8,2,1.2,2.4,2.6,3.5,1.3,1.7,4.8,2.9,5.2,5.8,2.6,3.8,4.1,2.2,0.1,2.3,4,3.8,9.1,5.5,0.9,3,6.2,1.9,2,6,1.6,3.7,0.8,5,-0.6,2.7,2.6,6,3.1,2.8,-0.5,1.4,3,1.1,2.8,1.1,6.9,0.9,3.1,4,1.7,1.9,8.2,0.7,3.7,4.6,2.7,1.3,2.1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>gm<\/th>\n      <th>inf<\/th>\n      <th>gx<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
```


## Plotting Graphs

Let us see if there are any clear relationships between the dependent and independent variables in our data

### Plotting Growth Rate of Money Supply to Inflation


```r
# plot scatter plot then add line of best fit using lm
# {} is used to tell R to run the whole chunk at once

{plot(money$gm, money$inf, main = "Growth Rate of Money Supply", 
     xlab = "Money Supply", ylab = "Inflation" )
  abline(lm(inf ~ gm, money), col = 'blue', lty = 'dashed')} 
```

![](Econ_Jad_Istanbelly_files/figure-html/unnamed-chunk-28-1.png)<!-- -->
Disregarding any obvious outliers, the majority of our data is clustered together with small positive increases to Inflation when Money Supply increases hence a positive/direct relationship is present.


### Plotting Growth Rate of Real GDP to Inflation


```r
# plot scatter plot then add line of best fit using lm
# {} is used to tell R to run the whole chunk at once

{plot(money$gx, money$inf, main = "Growth Rate of Real GDP", 
     xlab = "Real GDP", ylab = "Inflation")
  abline(lm(inf ~ gx, money), col = 'blue', lty = 'dashed')}
```

![](Econ_Jad_Istanbelly_files/figure-html/unnamed-chunk-29-1.png)<!-- -->
Disregarding any obvious outliers, the majority of our data seems to decrease Inflation when Real GDP increases hence a negative/indirect relationship is present.



## Quantity Theory

According to the Quantity Theory: 

$$
\beta_{2}=1, \beta_{3}=-1
$$
Therefore the regression function would look like: 
$$
\mathrm{INF}=\beta_{1}+\mathrm{GM}-\mathrm{GX}+\mathrm{e}
$$
To test whether our data supports this we can run a F-test with the following Hypotheses: 

$$
H_{0}: \beta_{2}=1, \beta_{3}=-1
$$
$$
H_{1}: \beta_{2} \neq 1, \beta_{3} \neq -1
$$


### Does the data support the Quantity Theory?


```r
# Set Alpha level
alpha <- 0.05

# Create Hypothesis Statement
hypo <- c("gm = 1", "gx = -1")

# Run Regression Function
model <- lm(inf ~., data = money)

# Compute and Extract F-Statistic
fstat <- linearHypothesis(model,hypo)$`Pr(>F)`[2]

# Find P-Value for given F-stat and DF
pval <- 1-pf(fstat, 2, model$df.residual)

# If Statement for Final Results
if(pval < alpha){
  print("We reject the Null and conclude that the model is significantly different from 1 and -1. Therefore, Quantity Theory does not hold")
}else {print("We fail to reject the Null and conclude that the model is not significantly different from 1 and -1. Therefore, Quantity Theory does hold")}
```

```
## [1] "We fail to reject the Null and conclude that the model is not significantly different from 1 and -1. Therefore, Quantity Theory does hold"
```



## Testing a Restriction

The following restriction is given:
$$
\beta_2 + \beta_3 = 0
$$
We can rewrite the model to represent this restriction as follows: 
$$
\mathrm{INF}=\beta_{1}+(\beta_{2}+\beta_{3}) \mathrm{GM}+\beta_{3} \mathrm({GX-GM})+\mathrm{e}
$$
Now we can test using a F-test, the hypotheses are listed below:

$$
H_{0}: \beta_{2} +\beta_{3}=0
$$

$$
H_{1}: \beta_{2} + \beta_{3} \neq 0
$$








```r
# Set Alpha level
alpha <- 0.05

# Create Hypothesis Statement
hypo <- c("gm = 0", "gx = 0" )

# Run Regression Function
model_r <- lm(inf ~ gm + gm + (gx - gm), data = money)

# Compute and Extract F-Statistic
fstat <- linearHypothesis(model_r,hypo)$`Pr(>F)`[2]

# Find P-Value for given F-stat and DF
pval <- 1-pf(fstat, 2, model$df.residual)

# If Statement for Final Results
if(pval < alpha){
  print("We reject the Null and conclude that the restricted model is not supported at the 5% level of significance")
}else {print("We fail to reject the Null and conclude that the restricted model is supported at the 5% level of significance")}
```

```
## [1] "We fail to reject the Null and conclude that the restricted model is supported at the 5% level of significance"
```

## Testing for Autocorrelation

Using the Durbin-Watson test we can check for autocorrelation

$$ H_0: \rho_{1}=\rho_{2}=\cdots=\rho_{p}=0$$
$$ H_1: \rho_{1}\neq\rho_{2}\neq\cdots\neq\rho_{p}\neq0$$

### Durbin-Watson Test

```r
# Set Alpha level
alpha <- 0.05

# Extract Statistic from DW test

dwstatistic <- dwtest(model)$statistic

# Check DW Statistic

if( 1.5 < dwstatistic && dwstatistic < 2.5){
  print("Relatively normal value")
  } else {print("Potentially concerning value")} 
```

```
## [1] "Relatively normal value"
```

```r
# Extract P-value from test

pval_dw <- dwtest(model)$p.value

# Hypothesis Test

if(pval_dw < alpha){
  print("We reject the Null and conclude that there is autocorrelation in the residuals of our model")
  } else {print("We fail to reject the Null and conclude that there is no autocorrelation in the residuals of our model")}
```

```
## [1] "We fail to reject the Null and conclude that there is no autocorrelation in the residuals of our model"
```

# Wage and Years of Schooling

This dataset explores the relationship between wages and schooling. A dummy variable model will be applied in this case.


```r
remove(list = ls())

library(haven)
wageedu <- read_dta("C:/Users/jadel/OneDrive - Saint Marys University/Econometrics/Final/DataSets/wageedu.dta")
datatable(wageedu)
```

```{=html}
<div id="htmlwidget-f4585ccfe62ca04366cd" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-f4585ccfe62ca04366cd">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100"],[308,308,308,308,326,326,326,326,326,326,326,326,326,326,326,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,731,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123,1123],[6,6,8,8,6,6,8,8,8,12,12,12,12,12,12,6,8,8,8,8,8,12,12,12,12,12,12,12,12,12,12,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,12,12,12,12,12,12,12,12,12,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16,16]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>wages<\/th>\n      <th>schooling<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
```

## Conditional Means E(Y|X)

We can find E(Y|X) using two different methods: 

1) Running a simple linear regression then finding the predicted values as the definition of E(Y|X) is Yhat in this case
2) Create Dummy variables to represent various X data points then run a simple linear regression without the intercept/constant as the definition of E(Y|X) is the beta coeffecients in this case.

### Simple Linear Regression

```r
# Create DF  

df <- wageedu

# Run Simple Linear Regression  

model <- lm(wages ~. , df)

# Extract Yhat for each different X-value

yhat <- unique(round(model$fitted.values, 2))

# Combine Yhat with Corresponding X-Value into a simple matrix

cm <- rbind(unique(df$schooling), yhat)
cm <-`rownames<-`(cm, c("X","E(Y|X)"))

cm
```

```
##          [,1]   [,2]   [,3] [,4]
## X        6.00   8.00  12.00   16
## E(Y|X) 399.83 524.87 774.93 1025
```

### Dummy Variables

```r
# Create Dummy Variables for each different X-value
# 1 = True 0 = False

D1 <- ifelse(wageedu$schooling == 6, 1, 0) # If X is 6 then set as 1 if not then set as 0
D2 <- ifelse(wageedu$schooling == 8, 1, 0) # If X is 8 then set as 1 if not then set as 0
D3 <- ifelse(wageedu$schooling == 12, 1, 0) # If X is 12 then set as 1 if not then set as 0
D4 <- ifelse(wageedu$schooling == 16, 1, 0) # If X is 16 then set as 1 if not then set as 0

# Create new DF with Dummy Variables

df1 <- data.frame(wageedu[ , 1], D1, D2, D3, D4)

# Run Regression using Dummy Variables without Intercept

model1 <- lm(wages ~. -1 , df1)
model1$coefficients
```

```
##      D1      D2      D3      D4 
##  399.80  524.90  774.92 1025.00
```
### Compare both methods
To easily compare both methods let us create a basic matrix with our results

```r
# Combine Results from both methods into a matrix 

compare <- rbind(cm[2, ], model1$coefficients)

# Change Row names

compare <-`rownames<-`(compare, c("E(Y|X)", "Dummy"))

# Change Column names

compare <- `colnames<-`(compare, c("D1=6", "D2=8", "D3=12", "D4=16" ))

compare
```

```
##          D1=6   D2=8  D3=12 D4=16
## E(Y|X) 399.83 524.87 774.93  1025
## Dummy  399.80 524.90 774.92  1025
```

## Average wage between 12 and 16 years of schooling

Is the average wage for those who have 12 years of schooling significantly different than that for those with 16 years of schooling?

To answer this we can run a T-test where d represents the difference between the two means $$\mu_{12} -\mu_{16} = \mu_{d}$$


Therefore, we can state the hypotheses as follows:
$$
\begin{array}{l}
H_{0}: \mu_{d}=0 \\
H_{1}: \mu_{d} \neq 0
\end{array}
$$

### T-Test 

```r
# Set Alpha level
alpha <- 0.05

# Run T-test 
t_test <- t.test(D3, D4, data = df1, paired = TRUE)

if(t_test$p.value < alpha ){
  print("We reject the Null and conclude that the average wage of 12 years of schooling is significantly different than 16 years")
} else {print("We accept the Null and conclude that the average wage of 12 years of schooling is not significantly different than 16 years")}
```

```
## [1] "We reject the Null and conclude that the average wage of 12 years of schooling is significantly different than 16 years"
```

# Utown

This dataset lists the prices of houses based on the following characteristics: 

1) Square feet (Numeric Value in 100s)
2) Age (Numeric Value in years)
3) Is it a University Town? (Binary Value)
4) Does it have a pool? (Binary Value)
5) Does it have a fireplace? (Binary Value)

Binary Variables are handled in the following format: 

+ 1 for yes 
+ 0 for no


```r
remove(list = ls())

library(haven)
utown1 <- read_dta("C:/Users/jadel/OneDrive - Saint Marys University/Econometrics/Final/DataSets/utown1.dta")
datatable(utown1)
```

```{=html}
<div id="htmlwidget-39883c409427182bf5db" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-39883c409427182bf5db">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428","429","430","431","432","433","434","435","436","437","438","439","440","441","442","443","444","445","446","447","448","449","450","451","452","453","454","455","456","457","458","459","460","461","462","463","464","465","466","467","468","469","470","471","472","473","474","475","476","477","478","479","480","481","482","483","484","485","486","487","488","489","490","491","492","493","494","495","496","497","498","499","500","501","502","503","504","505","506","507","508","509","510","511","512","513","514","515","516","517","518","519","520","521","522","523","524","525","526","527","528","529","530","531","532","533","534","535","536","537","538","539","540","541","542","543","544","545","546","547","548","549","550","551","552","553","554","555","556","557","558","559","560","561","562","563","564","565","566","567","568","569","570","571","572","573","574","575","576","577","578","579","580","581","582","583","584","585","586","587","588","589","590","591","592","593","594","595","596","597","598","599","600","601","602","603","604","605","606","607","608","609","610","611","612","613","614","615","616","617","618","619","620","621","622","623","624","625","626","627","628","629","630","631","632","633","634","635","636","637","638","639","640","641","642","643","644","645","646","647","648","649","650","651","652","653","654","655","656","657","658","659","660","661","662","663","664","665","666","667","668","669","670","671","672","673","674","675","676","677","678","679","680","681","682","683","684","685","686","687","688","689","690","691","692","693","694","695","696","697","698","699","700","701","702","703","704","705","706","707","708","709","710","711","712","713","714","715","716","717","718","719","720","721","722","723","724","725","726","727","728","729","730","731","732","733","734","735","736","737","738","739","740","741","742","743","744","745","746","747","748","749","750","751","752","753","754","755","756","757","758","759","760","761","762","763","764","765","766","767","768","769","770","771","772","773","774","775","776","777","778","779","780","781","782","783","784","785","786","787","788","789","790","791","792","793","794","795","796","797","798","799","800","801","802","803","804","805","806","807","808","809","810","811","812","813","814","815","816","817","818","819","820","821","822","823","824","825","826","827","828","829","830","831","832","833","834","835","836","837","838","839","840","841","842","843","844","845","846","847","848","849","850","851","852","853","854","855","856","857","858","859","860","861","862","863","864","865","866","867","868","869","870","871","872","873","874","875","876","877","878","879","880","881","882","883","884","885","886","887","888","889","890","891","892","893","894","895","896","897","898","899","900","901","902","903","904","905","906","907","908","909","910","911","912","913","914","915","916","917","918","919","920","921","922","923","924","925","926","927","928","929","930","931","932","933","934","935","936","937","938","939","940","941","942","943","944","945","946","947","948","949","950","951","952","953","954","955","956","957","958","959","960","961","962","963","964","965","966","967","968","969","970","971","972","973","974","975","976","977","978","979","980","981","982","983","984","985","986","987","988","989","990","991","992","993","994","995","996","997","998","999","1000"],[205.452,185.328,248.422,154.69,221.801,199.119,272.134,250.631,197.24,235.755,189.639,227.008,248.256,171.885,163.764,261.254,239.828,201.591,225.566,227.178,232.894,254.58,229.38,193.207,222.179,226.322,213.106,249.03,232.088,229.523,201.72,216.753,212.301,222.973,205.362,247.932,211.983,134.316,200.07,215.343,228.697,229.896,189.653,210.361,261.532,244.085,224.161,254.577,201.623,258.442,211.424,230.352,235.324,249.912,158.163,244.091,214.982,207.261,189.663,212.396,215.85,233.102,147.62,246.053,204.246,188.848,199.906,268.209,262.5,190.134,214.761,169.271,225.427,184.506,251.076,205.255,184.546,199.333,232.931,213.714,253.611,223.723,209.033,219.741,176.065,245.132,160.305,194.806,184.914,218.771,240.763,202.903,218.649,218.969,210.638,203.894,217.342,208.681,225.098,217.151,228.273,220.803,192.231,231.082,264.382,259.221,241.352,240.429,166.8,263.28,208.78,254.205,228.363,212.381,164.924,250.793,240.858,168.758,220.285,206.138,204.862,224.61,270.397,194.297,199.157,205.504,235.548,218.334,192.512,197.923,190.49,229.363,226.629,255.153,200.369,257.29,258.558,238.405,238.034,181.041,191.366,186.388,201.364,237.306,188.401,185.057,271.082,196.602,212.204,214.785,211.934,193.11,274.829,235.868,183.526,228.135,261.443,194.472,218.892,236.398,182.501,225.062,258.697,207.265,224.451,245.333,193.73,142.378,165.304,218.846,183.795,269.914,174.32,196.662,241.577,257.435,234.855,204.851,222.725,197.573,251.093,168.13,231.538,195.507,239.18,192.815,191.576,248.843,156.317,228.595,220.761,196.747,165.61,264.443,208.124,273.708,242.803,196.422,201.428,249.407,221.061,242.758,224.18,199.127,226.797,254.058,234.693,218.159,186.512,212.751,234.216,183.862,194.283,224.763,264.556,247.663,158.754,188.329,224.746,196.745,183.698,206.708,192.589,212.827,218.886,207.358,195.303,227.416,222.315,226.419,193.529,203.693,193.909,249.317,231.508,199.659,204.344,199.207,243.807,212.158,208.431,219.924,168.373,195.901,220.179,214.847,225.247,232.198,213.44,241.336,196.108,182.041,222.32,176.126,229.529,246.093,198.145,206.606,214.153,242.796,237.51,193.209,175.698,234.117,182.306,228.916,192.474,216.448,255.206,244.635,207.095,217.304,220.754,248.497,193.631,247.45,235.738,221.86,216.931,199.257,163.901,210.163,226.375,263.267,246.534,173.39,206.127,240.196,227.64,259.144,179.296,234.779,220.916,182.285,203.122,206.863,240.265,245.806,242.958,276.977,212.274,178.336,217.621,177.512,199.615,216.444,243.167,171.927,228.585,247.754,205.887,185.622,230.412,251.125,208.542,226.984,233.9,175.365,251.772,245.74,201.008,224.544,226.995,206.227,183.531,236.246,217.663,252.483,211.39,211.295,211.09,239.339,217.144,199.747,255.268,194.652,190.959,198.391,191.055,195.382,193.688,196.818,226.832,200.453,206.714,216.116,196.773,219.493,241.456,205.013,243.643,238.966,233.503,181.183,183.354,204.379,216.027,225.504,203.49,190.034,261.775,236.614,241.603,223.249,169.441,178.039,192.171,202.603,231.246,178.393,173.874,214.573,234.339,218.802,208.266,198.009,243.41,261.68,259.895,201.397,236.827,209.807,222.883,170.703,205.576,253.066,195.533,264.539,141.62,237.703,200.547,229.252,190.738,233.794,188.718,242.705,224.595,189.935,243.493,209.825,210.047,248.899,197.34,225.391,211.685,176.389,157.092,175.387,196.89,206.163,248.939,242.799,196.655,209.316,203.558,215.265,164.164,218.107,210.739,185.876,163.64,210.2,201.803,248.359,219.489,213.47,197.404,196.459,221.649,204.477,204.304,268.778,206.656,208.605,199.141,224.986,235.637,254.065,175.011,213.498,222.738,215.275,224.872,220.919,219.615,179.265,205.722,203.888,195.428,223.44,202.091,163.156,224.728,180.468,211.69,229.75,232.924,252.741,184.303,193.296,243.107,209.668,202.905,259.662,240.157,214.948,192.086,276.049,258.411,240.553,217.298,175.737,252.226,221.387,249.342,211.492,193.527,226.952,229.385,209.213,238.446,276.964,268.865,285.714,318.696,315.949,299.161,279.215,283.058,315.574,277.16,294.351,335.751,241.434,275.03,191.57,297.943,263.244,332.632,282.328,314.65,288.556,302.834,247.82,269.971,292.926,278.562,272.666,288.546,280.678,292.002,316.945,245.652,303.18,230.159,263.736,241.85,263.677,236.986,253.671,294.203,251.827,275.814,264.473,321.994,331.387,230.74,271.396,319.381,272.113,231.299,294.513,232.502,298.393,251.004,243.559,231.123,307.458,264.143,294.694,272.723,322.561,235.988,275.24,261.398,274.976,297.377,256.464,244.501,300.019,272.272,327.375,261.675,270.9,234.11,297.037,311.308,292.182,230.13,292.797,274.673,322.309,325.844,332.222,286.011,265.853,311.508,244.518,309.598,304.183,286.62,313.864,258.155,311.411,276.02,326.562,297.984,332.132,311.766,278.195,320.154,292,319.391,260.721,288.056,262.457,258.378,287.847,220.1,309.471,291.562,291.618,234.699,296.093,280.813,345.197,245.529,313.758,237.907,299.106,255.304,230.676,239.669,261.533,239.036,267.882,215.748,326.772,291.352,335.253,254.348,231.853,261.505,266.424,282.281,326.312,317.031,252.612,288.376,256.857,334.489,296.707,260.74,237.086,329.438,291.329,278.232,294.52,311.009,302.619,328.443,289.32,259.053,277.149,273.643,277.497,246.982,293.552,283.246,314.76,219.882,328.536,266.632,333.81,303.597,267.986,293.946,289.183,304.316,291.595,255.993,278.362,273.587,294.701,226.548,256.448,301.851,298.931,282.195,208.543,246.009,303.633,317.283,236.44,244.648,310.47,265.992,261.838,325.661,263.909,290.939,234.704,332.657,250.022,282.71,268.618,269.468,304.345,207.207,288.652,310.538,266.88,272.463,262.302,291.942,273.108,339.688,251.353,255.986,319.68,230.834,316.505,222.851,262.649,252.678,288.536,299.286,253.336,249.099,291.379,235.334,295.222,285.943,269.181,266.615,277.486,286.571,267.685,271.464,250.093,279.049,241.642,288.85,267.377,318.162,285.036,296.336,279.292,306.762,295.517,239.256,292.243,291.009,329.202,296.203,199.319,244.535,276.68,220.502,307.056,238.523,275.191,234.326,259.221,257.052,201.653,238.141,291.237,249.567,252.68,266.766,255.684,283.802,335.285,306.113,275.734,267.148,282.63,334.657,252.294,292.076,319.505,314.616,249.063,294.925,291.309,297.881,281.746,293.103,260.74,310.927,315.383,281.308,289.668,236.474,212.175,234.687,337.338,243.151,244.599,253.505,268.941,249.715,248.466,251.715,318.512,276.058,280.62,253.541,265.322,285.063,299.452,276.008,284.448,290.407,268.747,261.45,217.031,257.007,262.924,237.34,237.933,265.116,310.849,242.675,297.231,325.827,324.482,286.474,271.434,305.64,227.714,241.306,309.567,321.003,309.854,257.872,289.689,271.311,284.074,314.476,313.562,310.701,233.889,315.897,257.927,334.294,256.719,291.248,285.183,273.661,254.754,317.373,315.034,315.645,257.47,304.922,240.99,267.009,305.649,322.163,255.474,298.95,281.685,305.568,255.76,269.752,280.052,287.45,251.713,287.427,278.213,303.941,249.852,271.043,321.938,243.976,291.471,230.25,254.817,270.913,286.525,325.964,238.329,218.419,307.204,298.685,259.659,259.661,283.695,276.198,276.565,270.078,273.737,274.404,312.033,243.123,218.923,254.613,325.941,260.66,271.435,264.655,254.205,231.887,230.332,254.989,242.386,246.6,290.92,217.746,258.242,285.525,295.336,248.924,250.205,303.88,279.201,255.892,280.204,273.996,222.599,237.292,285.5,302.848,295.995,309.565,230.576,268.781,310.987,281.088,280.991,252.642,245.108,269.783,272.072,226.653,254.926,340.708,291.571,260.97,305.098,272.985,287.422,281.904,235.313,267.043,294.617,218.981,288.411,270.633,251.025,304.724,306.878,221.809,261.549,262.606,268.59,266.486,281.931,283.451,259.898,247.883,310.41,239.653,253.671,325.998,240.229,287.428,266.97,245.859,286.211,242.528,319.01,207.114,203.176,293.677,316.032,311.06,244.157,261.355,248.36,326.18,271.955,293.624,262.69,244.637,237.859,258.932,284.58,272.333,315.908,325.935,287.123,259.953,277.035,323.482,325.319,323.827,246.206,337.441,304.359,238.551,309.243,224.904,232.099,250.714,298.633,290.145,283.935,245.003,273.303,341.259,285.171,322.902,340.719,309.034,273.307,274.308,276.319,287.339,255.325,301.037,264.122,253.392,257.195,338.295,263.526,300.728,220.987],[23.46,20.03,27.77,20.17,26.45,21.56,29.91,27.98,24.8,27.5,20.82,23.38,29.82,20.06,20.6,29.1,29.54,23.11,27.03,25.7,27.61,29.53,25.13,23.4,25.45,27.27,22.81,27.93,26.6,28.05,23.74,25.72,26.16,25.94,23.2,28.35,24.42,20.47,21.27,27.16,28.06,25.76,22.57,26.96,27.58,26.32,24.95,29.27,21.69,29.74,25.48,29.34,26.96,29.86,21,28.76,22.5,24.7,23.96,22.57,25.5,27.23,20.47,29.16,27.01,20.16,21.91,28.9,29.3,22.46,25.82,21.55,26.23,25.64,25.8,20.99,21.26,22.3,25.8,25.36,27.38,24.37,25.71,25.33,20.38,29.82,20.08,20.69,22.14,23.63,29.04,27.49,27.73,23.67,24.25,24.6,23.36,24.35,27.33,25.2,24.17,26.72,22.68,25.18,29.08,29.08,27.88,27.38,20.07,29.44,23.51,29.52,25.38,24.61,22.27,29.27,25.96,21.14,26.78,24.75,25.3,25.51,29.55,22.47,26.08,24.81,28.77,27.24,22.89,22.15,25.15,27.72,25.52,29,23.3,29.08,27.37,25.26,28.92,20.95,23.77,21.29,24.97,28.57,20.41,21.91,29.86,25.73,28.21,27.54,20.1,21.88,29.91,26.83,20.17,28.2,25.7,24.47,24.46,27.44,22.93,28,29.09,22.24,22.34,28.42,20.98,22,21.63,27.73,22.08,29.56,20.33,23.4,25.82,29.85,25.96,22.69,27.46,22.95,27.81,21.1,27.84,21.63,27.27,22.4,20.17,26.4,20.5,27.76,25.59,23.31,20.81,27.75,25.09,28.4,28.23,21.93,20.77,29.38,23.73,28.11,27.6,20.29,25.88,28.03,28.26,26.22,21.37,27.2,29.71,24.26,21.6,26.54,25.94,29.9,23.9,21.81,27.13,22.89,21.12,24.05,22.86,23.75,26.32,26.74,20.61,27.71,24.04,27.68,23.18,20.9,22.05,28.12,28.65,26.61,20.98,23.61,29.46,24.77,23.96,26.24,22.42,22.25,25.56,25.29,26.83,28.81,27.25,28.99,23.7,20.96,26.13,21.99,24.87,28.84,22.7,25.6,26.13,28.51,26.18,23.95,22.88,26.64,20.74,28.93,20.92,29.52,28.98,25.73,22.01,26.57,23.8,26.86,23.88,28.44,28.57,23.29,27.42,24.51,22.4,25.62,27.01,27.21,29.83,21.37,25.25,24.75,27.77,27.99,20.28,25.62,23.95,20.99,23.43,26.87,26.53,27.8,26.52,28.04,25.84,21.12,26.27,20.15,25.91,26.31,29.16,22.2,23.69,29.05,22.93,21.97,23.43,27.75,24.17,22.67,24.62,20.35,27.87,28.21,25.3,24.46,22.76,21.38,22.74,26.24,26.45,27.37,26.63,25.36,25.23,27.17,22.71,20.4,28.94,22.01,25.55,21.51,27.06,23.67,23.76,21,28.65,22.08,25.21,23.14,23.26,22.13,27.85,24.51,28.95,26.59,26.72,20.59,21.39,20.9,20.31,28.74,22.59,23.5,28.88,24.96,29.39,27.05,20.16,20.23,21.63,24.76,26.16,22.22,21.97,24,28.71,28.63,26.28,23.08,29.37,28.32,29.89,26.75,27.12,24.11,25.92,24.25,25.61,26.08,23.2,29.02,20.03,26.92,25.17,27.46,25.18,24.11,25.31,29.73,27.33,21.07,26.81,25.23,26.34,28.37,22.25,27.86,28.55,21.7,20.31,21.28,20.03,23.4,29.48,29.75,20.45,22.16,24.91,25.83,20.97,23.57,28.25,22.87,21.15,23.06,23.37,27.51,28.83,25.69,23.55,22.89,28.99,22.76,24.25,29.68,23.42,24.23,23,24.7,28.61,26.38,20.64,25.18,21.02,27.5,25.1,25.5,29.15,22.84,20.98,21.48,23.07,26.4,24.62,20.23,25.38,21.07,21.77,26.17,27.09,27.8,25.44,23.15,27.28,25.95,25.37,29.06,29.28,26.58,26.66,29.77,29.25,28.94,26.49,20.27,28.53,25.77,28.96,25.42,24.2,22.06,28.22,24.93,27.21,24.26,24.45,28.28,29.4,28.07,25.87,24.42,24.58,29.78,24.7,26.87,27.7,20.89,25.21,20.08,26.74,24.76,29.45,23.99,29.28,24.48,27.02,21.26,22.76,26,21.73,26.08,26.81,27.12,25.55,29.39,21.17,27.74,20.04,27.18,22.94,22.24,23.71,22.59,25.92,22.69,23.55,21.28,29.91,29.79,21.1,27.28,28.18,25.14,22.3,24.88,20.44,28.97,23.91,20.06,20.34,29.69,25.11,27.13,26.84,29.83,21.84,24.88,22.95,22.44,28.57,21.56,20.43,27.46,25.77,27.28,24.24,21.61,22.21,26.23,27.7,26.5,20.31,25.99,28.43,29.44,25.77,29.64,27.22,22.25,29.48,24.14,28.33,29.07,29.61,29.07,22.43,29.18,26.08,28.61,26.57,29.99,28.55,24.31,28.78,26.5,29.58,22.12,29.71,25.83,24.85,26.64,20.49,27.25,28.37,28.07,23.33,25.39,23.48,29.85,22.02,29.21,20.06,25.89,21.32,22.32,21.33,22.78,22.38,23.62,21.13,29.54,26.63,29.5,23.65,21.52,23.28,23.88,28.44,28.14,25.04,23.82,27.37,20.81,29.93,26.57,23.31,21.95,29.73,26.24,27.55,26.39,29.95,27.29,29.94,25.76,23.12,27.31,28.37,23.47,24.93,29.91,26.26,29.72,20.21,29.47,24.24,29.49,29.09,26.18,28.31,23.75,28.37,29.3,22.95,24.74,24.61,26.58,20.39,23.96,29.11,25.92,28.06,20.39,22.75,29.89,28.06,22.86,22.56,28.49,23.35,21.9,27.89,22.21,26.45,20.35,29.52,23.82,27.56,23.53,22.98,28.37,20.2,29.45,29.03,25.87,26.55,25.5,28.8,26.7,29.87,22.79,21.62,27.45,20.71,29.59,22.14,24.3,21.99,24.41,26.76,23.44,25.24,27.06,20.26,28.36,25.3,25.17,23.67,24.19,25.79,24.89,24.5,23.03,23.28,22.25,28.72,25.67,29.29,24.93,29.47,27.57,26.91,26.57,23.01,26.3,27.98,27.97,27.14,21.38,22.11,23.09,21.5,26.97,20.43,25.43,20.26,21.13,21.86,21.04,21.06,28.32,22.55,22.29,24.55,24.43,22.93,28.48,28.97,23.9,23.14,25.27,29.78,23,28.48,28.23,25.29,24.62,24.25,23.5,28.85,24.38,27.16,22.1,28.41,25.07,22.66,29.34,22.33,20.72,20.68,29.81,21.06,22.82,24.26,24.09,27.67,22.16,23.89,28.97,27.05,24.09,22.39,21.03,25.38,29.02,26.1,25.99,27.92,23.26,21.62,20.64,21.57,26.97,23.16,20.56,24.45,28.27,23.21,26,29.74,26.68,25.3,26.3,28.47,21.31,20.47,26.31,29.58,28.45,25.11,25.96,27.86,25.65,27.92,29.91,28.55,24.19,29.17,24.58,28.4,24.81,27.49,26.32,24.33,23.44,29.3,28.42,28.23,24.2,23.41,24.14,23.58,29.57,29.08,22.03,29.18,26.89,29.83,23.05,22.97,27.36,28.73,21.91,26.89,24.66,27.15,23.33,24.06,29.37,20.52,29.19,20.23,22.6,25.76,27.75,28.62,21.31,20.96,29.9,25.37,26.99,23.98,23.64,25.8,24.03,24.23,23.09,25.68,28.82,23.2,20.52,21.6,29.5,25.41,25.72,24.33,22.41,20.98,20.05,26.61,20.94,21.79,27.11,22.57,26.39,27.05,27.41,23.47,20.84,29.16,21.73,21.27,24.61,24.23,20.74,22.83,27.28,26.6,28.77,29.12,20.79,26.19,28.98,29.55,25.8,21.12,21.25,25.18,26.27,23.59,23.49,27.85,26.46,22.93,26.73,23.46,28.55,26.87,22.27,23.23,26.66,20.41,25.29,23.5,20.67,28.03,28.05,20.68,20.92,24.53,23.11,23.3,24.65,26.03,22.1,20.35,29.27,20.17,22.83,29.7,20.7,28.98,25.99,21.29,26.77,21.24,28.62,20.16,20.76,26.8,28.74,28.3,20.48,28.04,23.04,29.14,24.78,24.77,20.64,21.96,20.17,25.73,29.04,23.5,27.66,28.35,26.39,24.15,24.82,29.72,29.72,28.84,22.58,29.13,28.62,22.97,28.92,20.51,20.74,22.98,27.08,28.55,24,21.14,26.05,29.95,27.54,29.26,28.81,28.55,24.18,27.44,26.32,23.67,21.3,29.87,24.84,20.53,22.84,30,23.99,28.74,20.93],[6,5,6,1,0,6,9,0,0,0,14,12,10,1,2,2,17,26,7,2,5,1,9,2,4,1,9,5,5,5,16,22,14,3,2,35,2,17,5,2,9,3,3,30,10,7,10,19,1,28,0,7,11,4,14,4,6,12,21,36,8,7,11,3,2,17,0,8,30,15,6,5,11,24,4,0,27,2,18,3,10,17,16,29,12,14,8,2,11,2,4,16,7,4,1,0,17,2,17,5,3,5,7,15,6,3,18,2,9,9,2,5,22,2,8,24,7,10,2,8,9,6,12,11,7,2,2,15,45,12,3,42,1,0,4,11,0,8,2,13,5,5,3,5,23,2,13,6,5,1,0,8,0,1,6,7,0,3,1,1,4,8,3,2,5,39,1,5,2,1,7,3,1,40,2,18,20,9,3,10,23,6,4,4,2,13,0,13,17,2,9,9,21,4,4,4,0,0,6,2,6,3,6,0,9,3,0,2,3,29,31,26,29,5,1,3,35,5,1,14,44,4,8,20,1,0,14,8,4,7,7,0,15,27,6,2,4,21,4,2,4,34,23,7,5,23,1,1,2,7,1,3,2,13,20,7,6,6,4,7,3,3,3,20,13,11,23,11,18,0,7,2,10,0,18,10,7,3,6,10,12,26,25,35,34,19,0,0,26,24,3,7,0,13,7,6,4,6,2,5,11,2,0,13,37,4,9,6,3,1,23,2,1,4,13,11,8,8,4,1,29,4,26,3,13,8,2,2,3,9,11,12,10,23,19,0,20,2,49,1,17,14,6,3,8,12,2,13,12,33,0,1,5,18,6,11,3,13,25,2,14,11,21,4,11,3,3,1,0,0,32,6,12,37,13,6,21,5,4,22,20,7,38,11,2,1,4,9,20,17,15,19,16,18,18,5,17,20,3,3,6,15,1,7,0,26,30,18,16,6,7,17,1,1,14,1,3,1,11,5,2,18,16,13,5,4,2,13,8,9,29,7,23,3,9,22,7,6,6,3,11,19,1,13,35,11,3,1,0,5,3,60,11,7,7,8,7,16,2,10,12,0,1,22,9,15,17,6,1,11,0,21,1,1,7,22,9,3,19,39,1,1,5,8,0,9,1,6,3,4,0,9,0,11,6,15,1,14,0,3,24,4,1,2,4,17,34,14,5,0,2,18,14,15,2,9,2,6,8,12,5,3,8,12,31,0,7,1,9,13,15,19,9,4,1,0,5,1,5,3,5,6,2,1,9,11,14,0,13,1,33,26,15,34,13,3,19,2,4,5,10,7,2,5,9,6,6,36,4,12,12,43,17,3,4,4,1,13,15,13,13,10,2,5,16,24,4,14,17,2,0,33,2,5,4,13,10,6,8,8,3,11,17,4,8,13,33,10,5,10,13,0,7,1,4,31,15,1,6,3,1,6,25,7,19,0,2,13,0,1,30,20,12,3,1,0,22,9,5,7,32,1,14,9,11,2,3,3,16,25,0,2,10,8,0,25,18,6,11,7,8,5,2,50,2,8,2,4,9,14,34,14,7,3,22,5,5,10,20,6,5,7,17,3,12,23,8,7,13,4,42,11,7,3,20,6,9,5,7,1,1,12,1,0,49,4,5,26,5,2,7,1,2,34,1,1,5,15,21,12,45,22,3,20,2,1,3,8,3,4,9,10,10,3,5,2,6,8,10,18,5,1,0,0,2,18,17,18,5,7,5,21,1,8,4,3,6,5,1,19,3,10,6,3,0,33,8,15,8,12,2,7,10,2,4,5,30,1,0,4,21,12,8,2,2,1,6,7,11,2,4,0,1,19,4,5,6,10,6,16,0,14,0,0,7,7,0,3,1,35,0,25,15,39,0,12,5,1,10,23,8,3,9,2,13,6,10,16,2,0,5,5,0,16,7,4,11,7,18,8,9,12,1,1,20,18,12,29,40,4,6,9,7,28,5,0,21,8,0,10,0,4,18,11,21,4,5,9,4,12,5,9,14,35,4,3,7,28,10,18,3,13,19,1,13,2,7,3,2,8,13,20,14,1,16,0,4,3,10,7,1,3,2,3,14,10,6,7,12,19,0,12,2,2,1,10,12,7,29,19,9,6,12,0,4,13,9,22,2,5,11,48,10,2,23,3,10,19,2,10,0,1,3,28,3,7,2,3,0,1,16,2,0,3,0,3,1,1,3,15,14,10,4,7,3,9,4,42,5,1,0,3,2,10,31,21,0,3,26,3,1,6,0,0,3,0,0,29,3,20,28,0,6,4,1,4,11,6,9,2],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],[0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,1,1,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,1,0,1,0,1,0,1,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,1,0,0,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,0,1,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,1,0,0,0,0,1,0,0,1,1,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,1,1,0,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,1,1,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,1,0,1,0,1,0,0,0,0,0,0,1,0,0,1,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,1,0,1,0,1,1,0,0,0,0,0,0,0,1,1,1,0,0,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,1,0,0,0,1,0,1,0,0,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0,0,1,1,1,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,0,0,0,0,1,1,0,0,0,0,1,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0],[1,1,0,0,1,1,1,1,0,0,0,1,0,0,0,0,0,0,1,1,1,1,0,0,1,0,1,0,0,0,1,1,0,0,1,1,1,0,0,0,1,0,1,1,0,0,0,1,0,1,1,0,0,1,1,1,0,1,0,1,0,0,0,1,0,1,1,1,0,0,0,0,1,1,0,0,0,1,0,0,1,0,0,1,0,1,0,0,0,0,0,1,1,1,1,1,0,0,1,0,1,0,0,0,1,1,1,1,0,1,0,1,1,1,0,1,1,1,1,0,0,1,1,1,0,0,1,1,0,0,0,1,0,1,0,1,1,1,0,1,1,0,1,0,0,0,1,0,0,0,0,1,1,0,1,0,1,0,1,1,0,0,1,0,0,1,0,1,1,1,0,1,0,0,0,1,0,0,1,1,0,0,1,0,1,0,0,0,0,1,1,0,1,1,0,1,1,1,0,0,1,0,0,0,1,1,1,1,0,1,0,1,0,0,0,0,1,1,0,1,0,0,1,1,0,1,0,1,0,0,0,1,0,1,0,1,1,0,1,1,0,0,0,0,1,0,0,1,0,1,0,0,0,0,0,1,0,0,1,1,1,1,1,1,0,0,1,1,0,0,0,1,1,0,0,0,0,1,1,1,0,1,1,1,0,0,1,1,1,1,0,1,0,0,1,1,0,1,0,1,1,0,1,0,0,1,1,1,1,0,0,1,1,0,1,1,1,0,1,1,0,0,0,1,1,1,0,1,1,1,1,1,1,0,0,0,0,1,1,1,1,0,0,1,1,1,0,0,0,0,0,1,1,0,0,1,0,1,1,1,1,1,0,0,0,1,0,0,1,0,1,0,1,1,1,1,0,1,0,1,0,1,0,1,0,0,1,0,0,0,1,0,0,1,0,1,1,0,1,1,1,0,1,1,0,1,1,0,0,1,1,1,0,1,0,0,0,0,1,1,1,0,1,0,0,0,0,1,0,0,0,1,1,1,1,1,1,0,0,1,0,1,1,1,0,1,1,1,1,0,1,1,1,1,1,1,1,1,0,1,1,1,1,1,1,0,0,0,1,1,1,1,0,0,1,1,1,0,0,1,1,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,0,1,0,0,0,1,1,1,0,1,1,1,0,1,0,1,1,1,0,1,1,0,0,1,0,0,1,0,1,1,1,1,0,1,0,1,1,1,1,0,0,1,0,0,0,1,1,1,0,1,0,1,0,1,0,1,1,1,0,1,1,1,0,0,0,0,0,1,1,0,1,1,0,1,0,0,1,1,1,0,1,1,1,1,0,0,0,0,0,1,0,0,0,1,1,1,1,1,0,1,0,0,1,1,1,0,1,1,0,0,0,1,0,0,0,1,1,1,0,0,0,0,1,1,0,0,0,0,0,1,1,1,0,1,1,1,0,0,1,0,0,0,0,1,1,0,0,1,0,0,0,0,1,0,0,0,1,1,1,0,1,1,0,1,0,0,0,1,0,0,0,1,1,1,0,1,0,1,1,1,1,1,0,0,1,0,1,0,1,1,1,1,0,1,1,1,1,0,0,1,0,1,1,1,0,0,0,0,1,1,0,1,0,1,1,1,1,0,1,0,0,1,1,0,1,1,1,1,0,1,0,1,1,1,0,1,0,1,0,1,1,0,0,1,1,0,1,0,1,1,1,0,1,1,0,1,1,0,0,0,0,1,1,0,0,1,1,1,0,1,1,1,0,0,0,0,1,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,1,1,1,1,0,1,0,0,1,1,0,1,0,1,1,0,1,0,1,0,0,1,1,0,0,1,0,1,0,0,0,1,1,1,1,1,1,0,1,0,0,0,1,1,1,1,1,0,0,1,1,1,1,0,1,0,1,1,0,1,1,0,1,1,1,0,0,0,0,0,1,0,1,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,1,0,0,1,0,0,1,1,1,1,1,0,1,0,1,0,0,1,0,1,0,0,1,0,1,0,1,1,0,0,0,0,0,1,1,0,1,1,1,0,1,1,0,1,1,0,0,0,0,1,1,0,1,1,0,0,0,1,1,0,1,0,1,1,0,1,0,1,1,1,0,0,1,1,1,0,0,1,1,0,1,1,1,1,0,0,0,1,0,0,0,0,1,0,1,1,1,1,1,1,1,1,0,1,0,1,0,0,0,0,0,0,1,1,1,0,0,1,0,0,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>price<\/th>\n      <th>sqft<\/th>\n      <th>age<\/th>\n      <th>utown<\/th>\n      <th>pool<\/th>\n      <th>fplace<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
```

To investigate the affects of these characteristcs on house pricing we will run two different classification models: 

1) Linear Probablity  
2) Logistic Regression


First we must create a classification question out of our dependent variable "Price" before running any models. Let us go with the following: 

+ What characteristics explain the top 25% priced houses? Or in this case the top 250 priced houses?
+ What increases the probability of a house to be classified as top 25% in price? 

Price will be converted to a dummy variable representing the top 25% prices and will be classified as High. I will be identifying top 25% house prices with "highly priced houses" or other similar variations such as "most expensive houses" in writing. This is just an attempt to simplify certain descriptions.

Finally, The regression function will look like this: 

$$
\text { High }_{i}=\beta_{1}+\beta_{2} \text { age }_{2 i}+\beta_{3} \text { sqft }_{3 i}+\beta_{4} \text { pool }_{4 i}+\beta_{5} \text { fplace }_{5 i}+u_{t}
$$


## Preparing Data for Estimation


```r
# Create a DF to work with

df <- utown1

# Sort DF based on ascending order of price 

df <- df[order(df$price), ]

# Create a threshold representing bottom 75 percentile

th <- quantile(df$price, 0.75)

# Convert Prices that are greater than 75 percentile to 1 if lower then to 0

high <- ifelse(df$price >= th, 1, 0 )

# Update DF with new dummy Y variable and remove unused variables in analysis

df <- cbind(high, df[, c(-1, -4)])
```


## Linear Probability Model

### Estimating Model


```r
# Run Linear Probability Model

model <- lm(high ~. , df)
```


### Testing Significance of Coeffecients

Let us test the p-value of each variable to indicate whether it is significant and helpful in explaining what makes a high priced house.

```r
# Set Alpha level
alpha <- 0.05

# Find P-Value for each Beta disregarding intercept
pvals <- summary(model)[["coefficients"]][-1, 4]

# If Statement in a loop for each variable 
for (pval in pvals) {
  if(pval < alpha){
  print("Beta is statistically significant")
}else {print("Beta is not statistically significant")}
}
```

```
## [1] "Beta is statistically significant"
## [1] "Beta is not statistically significant"
## [1] "Beta is not statistically significant"
## [1] "Beta is not statistically significant"
```


### Significant Coeffecients Interpretation

Using this LPM model we can state that only squarefeet seems to be linked to highly priced houses. So to answer our classification question using this LPM model, we can state the following

```r
# Extract beta from model and create explanation 
beta <- percent(model$coefficients[2], accuracy = 0.01)
explanation <- "A house has a 7.1% increase in the probability of being a high priced house for every 100 square feet"

# Form a simple matrix with coeffecients and interpretation
results <- cbind(beta, explanation)
datatable(results)
```

```{=html}
<div id="htmlwidget-95b347f198c6d02561a8" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-95b347f198c6d02561a8">{"x":{"filter":"none","vertical":false,"data":[["sqft"],["7.10%"],["A house has a 7.1% increase in the probability of being a high priced house for every 100 square feet"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>beta<\/th>\n      <th>explanation<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>
```


  
## Logistic Regression Model

### Estimating Model


```r
# Run Logistic Regression Model

model1 <- glm(high ~., family = "binomial", df)
```


### Testing Significance of Coeffecients

A similar approach is done with Logistic Regression, where we we test the P-values in order to find the coefeccients that have statistical significance to highly priced houses

```r
# Set Alpha level
alpha <- 0.05

# Find P-Value for each Beta disregarding intercept
pvals_logit <- summary(model1)[["coefficients"]][-1, 4]

# If Statement for Final Results
for (pval_logit in pvals_logit) {
  if(pval_logit < alpha){
  print("Beta is statistically significant")
}else {print("Beta is not statistically significant")}
}
```

```
## [1] "Beta is statistically significant"
## [1] "Beta is not statistically significant"
## [1] "Beta is statistically significant"
## [1] "Beta is not statistically significant"
```


### Significant Coeffecients Interpretation
Interpreting Beta Coeffecients in this case is not as straight forward. However, using this Logistic model we can state that only squarefeet and pool seems to be linked to highly priced houses. So to answer our classification question using this logistic model, we can state the following

```r
# Extract beta from model and create explanation 
beta1 <- percent(model1$coefficients[c(2,4)], accuracy = 0.01)
explanation1 <- c("51.83% change in log of odds of 100 square feet", "41.36% change in the log of odds of having a pool")


# Form a simple matrix with coeffecients and interpretation
results1 <- cbind(beta1, explanation1)
results1 <- `colnames<-`(results1, c("beta", "explanation"))

datatable(results1)
```

```{=html}
<div id="htmlwidget-6be1331ec440f5f8abdb" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-6be1331ec440f5f8abdb">{"x":{"filter":"none","vertical":false,"data":[["sqft","pool"],["51.83%","41.36%"],["51.83% change in log of odds of 100 square feet","41.36% change in the log of odds of having a pool"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>beta<\/th>\n      <th>explanation<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>
```




## LPM vs Logit

We now know that our logistic model better explains highly priced houses due to more statistically significant predictors. However, we still have issues with understanding the coeffecients, this is where odd ratios step in. 

## Calculating Odd Ratios

It is easier to interpret the beta coeffecients for logistic regression using odds as it is simply raised by eulers number:  $e^{\beta}$


```r
library(oddsratio)

# Method 1 

# Find Odd Ratios with an increase of 1 unit per variable
OR <- or_glm(data = df , model = model1, incr = list(age = 1, sqft = 1, pool = 1, fplace = 1))

# Extract Predictor and Oddsratio Columns for specific predictors
OR <- OR[c(1,3) ,c(1, 2)]

# Create a new column representing interpretation of oddsratio
OR$explanation <- c("A house is 1.679 times more likely to be in the top 25% most expensive houses when its size increases by 100 square feet", "A house is 1.512 times more likely to be in the top 25% most expensive houses when it has a pool") 

# Method 2 
# exp(coef(model1))

datatable(OR)
```

```{=html}
<div id="htmlwidget-232d497e8a33521b2882" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-232d497e8a33521b2882">{"x":{"filter":"none","vertical":false,"data":[["1","3"],["sqft","pool"],[1.679,1.512],["A house is 1.679 times more likely to be in the top 25% most expensive houses when its size increases by 100 square feet","A house is 1.512 times more likely to be in the top 25% most expensive houses when it has a pool"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>predictor<\/th>\n      <th>oddsratio<\/th>\n      <th>explanation<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":2},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
```

We can also derive percentages from our odds by doing the following 
$$
(OR-1)*100
$$


```r
library(dplyr)

# create a copy of our table above and rename oddsratio to percentage

perc <- rename(OR, percentage = oddsratio)

# Convert Odds Ratio to Percentage 

perc$percentage <- percent((perc$percentage-1), accuracy = 0.01)

# Update explanation column 

perc$explanation <- c("A house with 100 square feet increase has a 67.90% more odds of appearing in the top most expensive houses", "A house with a pool has a 51.20% more odds of appearing in the top most expensive houses")

# Show results 

datatable(perc)
```

```{=html}
<div id="htmlwidget-e2d649ff5bc43638805c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-e2d649ff5bc43638805c">{"x":{"filter":"none","vertical":false,"data":[["1","3"],["sqft","pool"],["67.90%","51.20%"],["A house with 100 square feet increase has a 67.90% more odds of appearing in the top most expensive houses","A house with a pool has a 51.20% more odds of appearing in the top most expensive houses"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>predictor<\/th>\n      <th>percentage<\/th>\n      <th>explanation<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"order":[],"autoWidth":false,"orderClasses":false,"columnDefs":[{"orderable":false,"targets":0}]}},"evals":[],"jsHooks":[]}</script>
```


## Comparing Marginal Effects at Means

Arguably interpreting a model using Average Marginal Effects (AME) is an easier metric to understand. In simple terms, marginal effects is how the beta coeffecients are interpreted in linear regression. Where, the change of an independent variable increases or decreases the change in the dependent variable. 

Let us compare the marginal effects of our Logistic Model to the Beta Coeffecients of our LPM model

```r
library(margins)

# Calculate Marginal Effects of Logistic and extract Predictor and AME Column

MEM_Logit <- summary(margins(model1))[, c(1,2)]

# Extract Betas from LPM and set into DF to match MEM_Logit

MEM_LPM <- as.data.frame(model$coefficients[-1])

# Re-order rows to match ME_Logit

MEM_LPM <- MEM_LPM %>% arrange((row.names(MEM_LPM)))

# Find difference between the two models

MEM_diff <- abs(MEM_Logit[, 2] - MEM_LPM)

# Combine into DF to compare 

MEM_Compare <- cbind(MEM_LPM, MEM_Logit[, 2], MEM_diff)
colnames(MEM_Compare) <- c("Beta Coeffecients LPM", "Average Marginal Effect Logit", "Absolute Difference in Marginal Effect" )

# Round numbers for easy read

formatPercentage(datatable(MEM_Compare), columns = colnames(MEM_Compare), digits = 2) 
```

```{=html}
<div id="htmlwidget-be5f6b6b8b1c998033e3" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-be5f6b6b8b1c998033e3">{"x":{"filter":"none","vertical":false,"data":[["age","fplace","pool","sqft"],[-0.00180855780753084,-0.0118550769329024,0.0519492033023357,0.0710121445153413],[-0.0023389875895334,-0.0105603095521028,0.0575495759668801,0.0721124737436689],[0.000530429782002557,0.00129476738079959,0.00560037266454446,0.0011003292283276]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Beta Coeffecients LPM<\/th>\n      <th>Average Marginal Effect Logit<\/th>\n      <th>Absolute Difference in Marginal Effect<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"targets":1,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatPercentage(data, 2, 3, \",\", \".\");\n  }"},{"targets":2,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatPercentage(data, 2, 3, \",\", \".\");\n  }"},{"targets":3,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatPercentage(data, 2, 3, \",\", \".\");\n  }"},{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":["options.columnDefs.0.render","options.columnDefs.1.render","options.columnDefs.2.render"],"jsHooks":[]}</script>
```

As seen in the table above the difference between the Betas and AME is minimal making them almost identical. 


## Predicting Probabilites 

To predict probablities we need to:

1) Create a train and test subset from our dataset 
2) Recreate out model using the train data 
3) Predict probabliities using the test portion. 


```r
# Create Train and Test split

ind <- sample(nrow(df), nrow(df)*0.90, replace = FALSE)
train <- df[ind, ]
test <- df[-ind,]

# Run model on train

model2 <- glm(high ~ ., data = train, family = "binomial")

# Predict Probabilities on test

phat <- predict(model2, test, "response")

# Put predicted and Y in the same DF for easy use

predicted_df <- data.frame(phat, "Y" = test$high)
```

## Confusion Matrix

### False Positive Rate

The false positive rate informs us the percentage in which our model has predicted a positive outcome meanwhile the actual outcome is a negative. So for instance, our model determines a certain data point as a "highly priced house" meanwhile it does not actually classify it as one. A low False Positive Rate is not necessarily a good indicator that the model is succesful rather, we should look at other metrics.


```r
# Convert probablities to predicted outcome variable and create new column in predicted_df 

predicted_df$yhat <- ifelse(predicted_df$phat >= 0.1, 1, 0)

# Create confusion table

ctable <- table(predicted_df$yhat, predicted_df$Y)

# Function for a nicer looking confusion table
rot <- function(x){
  t <- apply(x, 2, rev)
  tt <- apply(t, 1, rev)
  return(t(tt))
}
ct <- rot(ctable)
rownames(ct) <- c("Yhat = 1", "Yhat = 0")
colnames(ct) <- c("Y = 1", "Y = 0")
ct
```

```
##           
##            Y = 1 Y = 0
##   Yhat = 1    31    33
##   Yhat = 0     2    34
```

```r
# False Positive Rate calculation 

fpr <- ct[1, 2]/(ct[1, 2] + ct[2, 2])
percent(fpr, accuracy = 0.01)
```

```
## [1] "49.25%"
```


### Important Metrics

Apart from the False Positive Rate, we can determine other metrics that help identify the performance of a model such as:

  + Accuracy = The percentage in which our model correctly classifies a high or non high house out of the total observations
  + Sensitivity = The percentage in which our model correctly classified a highly priced house out of all true high priced houses
  + Specificity = The percentage in which our model correctly classified a non highly priced house out of all true non high priced houses
  + Precision = The percentage in which our model correctly classified a highly priced house out of all of its predicted high priced houses (Whether truly a high priced house or not)


```r
# Instead of calculating the metrics manually another method to is to use the following package

library(caret)

# Convert to factor to run confusionMatrix

predicted_df$yhat <- as.factor(predicted_df$yhat)
predicted_df$Y <- as.factor(predicted_df$Y)

# Create Confusion Matrix and metrics
cm <- confusionMatrix(predicted_df$Y, predicted_df$yhat, positive = "1")

metrics <- c(cm$overall['Accuracy'], cm$byClass[c('Sensitivity', 'Specificity', 'Precision')]) 

final_metrics <- data.frame(metrics)

formatPercentage(datatable(final_metrics), columns = colnames(final_metrics), digits = 2)
```

```{=html}
<div id="htmlwidget-5ae7edb93c0ae2b0407a" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-5ae7edb93c0ae2b0407a">{"x":{"filter":"none","vertical":false,"data":[["Accuracy","Sensitivity","Specificity","Precision"],[0.65,0.484375,0.944444444444444,0.939393939393939]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>metrics<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"targets":1,"render":"function(data, type, row, meta) {\n    return type !== 'display' ? data : DTWidget.formatPercentage(data, 2, 3, \",\", \".\");\n  }"},{"className":"dt-right","targets":1},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":["options.columnDefs.0.render"],"jsHooks":[]}</script>
```


Running metrics to determine the success of our model once may not provide us with reproducible numbers to make a decision. For example, when we are splitting our data into “train” and “test” we are randomzing it and thus getting a different set of data points everytime. This kind of randomization can cause an inconsistency when calculating metrics:

First run we get more “highly priced houses” in our test
Second run we get less “highly priced houses” in our test
Third run our model miscalculates at a higher rate in our test
and so on..

In simpler terms, if you were to re-run all code chunks from the "Predicting Probablities" Section you will notice that our metric values change 

Therefore, it is common to loop our analysis hundreds or thousands of times then finding the average of our metrics for more consistent numbers


```r
# Most of this code has been copied and pasted from code chunks above
# Only new codes are annotated here

remove(list = ls())
library(haven)
utown1 <- read_dta("C:/Users/jadel/OneDrive - Saint Marys University/Econometrics/Final/DataSets/utown1.dta")

df <- utown1
df <- df[order(df$price), ]
th <- quantile(df$price, 0.75)
high <- ifelse(df$price >= th, 1, 0 )
df <- cbind(high, df[, c(-1, -4)])

# Empty containers are needed before loop starts 

ACC <- c()
SENS <- c()
SPEC <- c()
PREC <- c()

# Loop

for (i in 1:1000) {
  
  #cat("loops: ", i, "\r") # Counter to keep track of loops when running (remove the # beside cat to use)

  ind <- sample(nrow(df), nrow(df)*0.90, replace = FALSE)
  train <- df[ind, ]
  test <- df[-ind,]

  model2 <- glm(high ~ ., data = train, family = "binomial")
  phat <- predict(model2, test, "response")

  predicted_df <- data.frame(phat, "Y" = test$high)
  predicted_df$yhat <- ifelse(predicted_df$phat >= 0.1, 1, 0)
  
  predicted_df$yhat <- as.factor(predicted_df$yhat)
  predicted_df$Y <- as.factor(predicted_df$Y)
  
  cm <- confusionMatrix(predicted_df$Y, predicted_df$yhat, positive = "1")
  
  ACC[i] <- cm$overall['Accuracy'] 
  SENS[i] <- cm$byClass['Sensitivity'] 
  SPEC[i] <- cm$byClass['Specificity']
  PREC[i] <- cm$byClass['Precision']

}

# Average of each metric converted to percentage

MACC <- percent(mean(ACC), accuracy = 0.01)
MSENS <- percent(mean(SENS), accuracy = 0.01)
MSPEC <- percent(mean(SPEC), accuracy = 0.01)
MPREC <- percent(mean(PREC), accuracy = 0.01)

# Input Average of metrics into DF

metrics <- data.frame("Accuracy" = MACC, "Sensitivity" = MSENS, "Specificity" = MSPEC, "Precision" = MPREC)
```


## Is Our Model Successful?

As you can see below, we have 250 observations classified as a "high priced house" meanwhile 75% of our observations are classified as not. This is important to make note of before making a decision for the following reasons:


```r
df %>% count(high)
```

```
##   high   n
## 1    0 750
## 2    1 250
```

1) This causes an imbalance in our dataset and therefore The metric "Accuracy" is not as reliable in this scenerio.

2) It makes sense for our model to be more specific than sensitive. Due to the imbalance towards the negative class.

3) Arguably we'd prefer a higher "Precision" rate than "Sensitivity" rate to determine the correctness of our model and ensure that whatever we classify as a high priced house is truly one.

Now, let us take a look at our metrics.


```r
metrics
```

```
##   Accuracy Sensitivity Specificity Precision
## 1   60.63%      38.46%      96.92%    95.30%
```

With over 95% in our Average Metrics (Specificity and Precision) and using our reasonings above, I would state that our model is succesful in predicting highly priced houses. 





